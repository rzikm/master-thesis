\chapter{Analysis}\label{chap:03-analysis}

In this chapter, we analyze the protocol and select the necessary subset needed for evaluation of a
pure \dotnet{} implementation. Afterwards, we design the architecture and outline the implementation
\todo{this should be expanded once the chapter is written}

\section{Implemented Feature Subset Selection}

The set of features to be implemented is guided by the goals we set in this thesis goals in the
introduction chapter. For the purpose of feature selection, we can rephrase subset of the original
goals into the following:

\begin{enumerate}

  \item Support basic data transport, enabling some experimentation with QUIC as the transport for
    application layer protocols. \textit{(goals 1 and 2)}

  \item Enable performance measurements that are representative of the potential full QUIC
    implementation. \textit{(goals 1 and 3)}

\end{enumerate}

The first goal definitely requires full implementation of the multiplexed stream abstraction as
defined by the QUIC specification. It requires also implementing loss detection and recovery to
ensure that no data gets lost during the transport.

In order to get representative performance measurements, all performance affecting aspects of the
protocol should be implemented. The most important are packet protection and flow control because
they influence performance throughout the lifetime of the QUIC connection.

To summarize, the thesis should implement at least the following features:

\begin{itemize}

    \item Connection lifetime support (establishment, termination)

    \item Stream multiplexing

    \item Packet protection

    \item Loss detection and recovery

    \item Flow control

\end{itemize}

On the other hand, many QUIC features that react to one-time events can be disregarded, because
there either is no need for them in the evaluation environment, or they do not have relevant
performance or functional implications. In particular, implementation of the following features can
be avoided:

\begin{itemize}

    \item Connection migration, and therefore multiple Connection IDs support

    \item Complex (token-based) address validation

    \item Network path MTU detection

    \item Version negotiation

    \item 1-RTT key updates

    \item Advanced security measures (see Section 21 in transport specification~\cite{draft-ietf-quic-transport})

\end{itemize}

The rest of the features form a grey area which can be implemented fully, partially, or even not at
if convenient.

\section{Design Considerations}

Before we start the actual analysis, we will briefly outline the design principles used for the
actual design of the implementation and their rationale.

\subsection{Performance}

One of the key factors in the decision between managed \dotnet{} implementation of QUIC or using
external library like \libmsquic{} is performance. Therefore, the decisions made during the
implementation design should focus towards greater performance, possibly sacrificing maintainability
if the trade-off is justified.

As a general rule, the implementation will:

\begin{itemize}

    \litem{Avoid excessive heap allocations} Although heap allocation is cheap in \dotnet{} thanks
    to the garbage collector, allocating large number of objects may lead to frequent garbage
    collections. Each collection introduces a small stall into the program. Therefore,
    heap allocations on hot paths of the code executions should be minimized.

    \litem{Prefer return codes over exceptions} Throwing an exception is an expensive operation, and
    their frequent use would have negative impact on the performance.

\end{itemize}

\subsection{Testability}

The second design aspect we would focus on is testability of the implementation. Ideally, the design
would minimize the need for live debugging of the implementation. This is especially important
because stopping the implementation on a breakpoint will inevitably disrupt the connection, possibly
leading to termination because of a timeout.

The design intention is to allow writing automated tests that are able to inspect the packets sent
by the endpoint, and verify that they are consistent with the behavior defined by the QUIC
specification.

\section{Target \dotnet{} API}

This section describes the API that was designed by the \dotnet{} development team to expose QUIC to
other developers. This thesis will use this API to allow swapping underlying implementation between
this thesis implementation and the \libmsquic{}-based implementation. As mentioned in the
introduction chapter, the current design is a work-in-progress and is subject to change in the
future. All of the mentioned classes are located in the \namespace{System.Net.Quic} namespace.

\subsection{QuicListener Class}

The \class{QuicListener} class is the equivalent of the \class{TcpListener}. Servers use this
class to accept incoming QUIC connections.

\begin{description}

    \ditemctor{QuicListener}{QuicListenerOptions} Constructor.

    \ditemproperty{IPEndPoint}{ListenEndPoint}{\propget} The IP endpoint being listened to for new connection. Read-only.

    \ditemmethod{ValueTask<QuicConnection>}{AcceptConnectionAsync}{CancellationToken}
    Accepts a new incoming QUIC Connection.

    \ditemmethod{void}{Start}{} Starts listening.

    \ditemmethod{void}{Close}{} Stops listening and closes the listener. Does not close already accepted connections.

\end{description}

\subsection{QuicListenerOptions Class}

The \class{QuicListenerOptions} class holds all configuration used to construct new \class{QuicListener}s.

\begin{description}

    \ditemproperty{SslServerAuthenticationOptions}{ServerAuthenticationOptions}{\propgetset}
        SSL related options like certificate selection/validation callbacks, and supported protocols for ALPN\@.

    \ditemproperty{string}{CertificateFilePath}{\propgetset} Path to the X509 certificate used by the server.

    \ditemproperty{string}{CertificateKeyPath}{\propgetset} Path to the private key for the used X509 certificate.

    \ditemproperty{string}{CertificateKeyPath}{\propgetset} Path to the private key for the used X509 certificate.

    \ditemproperty{IPEndPoint}{ListenEndPoint}{\propgetset} The IP endpoint to listen on.

    \ditemproperty{int}{ListenBacklog}{\propgetset} Number of connection to be held waiting for acceptance by the application. Upon reaching this limit, further connections will be refused.

    \ditemproperty{long}{MaxBidirectionalStreams}{\propgetset} Limit on the number of bidirectional streams the client can open in an accepted connection.

    \ditemproperty{long}{MaxUnidirectionalStreams}{\propgetset} Limit on the number of unidirectional streams the client can open in an accepted connection.

    \ditemproperty{TimeSpan}{IdleTimeout}{\propgetset} The period of inactivity after which the connection will be closed via idle timeout.

\end{description}

\subsection{QuicConnection Class}

The \QuicConnection{} class provides operation on the QUIC connection. Clients open new
connections by creating a new instance of this class and calling the \method{ConnectAsync} method.
Servers receive new connections using the \class{QuicListener} class.

\begin{description}

    \ditemctor{QuicConnection}{QuicClientConnectionOptions} Constructor. The newly created instance is not connected until the call to \method{ConnectAsync} method.

    \ditemproperty{bool}{Connected}{\propget} Indicates whether the \QuicConnection{} is connected (the handshake has completed).

    \ditemproperty{IPEndPoint}{LocalEndPoint}{\propget} Local IP endpoint of the connection.

    \ditemproperty{IPEndPoint}{RemoteEndPoint}{\propget} Remote IP endpoint of the connection.

    \ditemmethod{ValueTask}{ConnectAsync}{CancellationToken} Connects to the remote endpoint.

    \ditemmethod{QuicStream}{OpenUnidirectionalStream}{} Opens a new unidirectional stream. Throws a \class{QuicException} if the stream cannot be opened.

    \ditemmethod{QuicStream}{OpenBidirectionalStream}{} Opens a new bidirectional stream. Throws a \class{QuicException} if the stream cannot be opened.

    \ditemmethod{ValueTask<QuicStream>}{AcceptStreamAsync}{Cancellationtoken} Accepts an incoming stream.

    \ditemmethod{ValueTask}{CloseAsync}{long, CancellationToken} Closes the connection with the specified given error code and terminates all active streams.

    \ditemmethod{long}{GetRemoteAvailableUnidirectionalStreamCount}{} Gets the maximum number of unidirectional streams that this endpoint can open.

    \ditemmethod{long}{GetRemoteAvailableBidirectionalStreamCount}{} Gets the maximum number of bidirectional streams that this endpoint can open.

\end{description}

\subsection{QuicClientConnectionOptions}

The \class{QuicClientConnectionOptions} is used by clients to configure new QUIC conections.

\begin{description}

    \ditemproperty{SslClientAuthenticationOptions}{ClientAuthenticationOptions}{\propgetset} Client authentication options to use when establishing the connection.

    \ditemproperty{IPEndPoint}{LocalEndPoint}{\propgetset} The local IP endpoint that will be bound to.

    \ditemproperty{IPEndPoint}{RemoteEndPoint}{\propgetset} The IP endpoint to connect to.

    \ditemproperty{long}{MaxBidirectionalStreams}{\propgetset} Limit on the number of bidirectional streams the server can open.

    \ditemproperty{long}{MaxUnidirectionalStreams}{\propgetset} Limit on the number of unidirectional streams the server can open.

    \ditemproperty{TimeSpan}{IdleTimeout}{\propgetset} The period of inactivity after which the connection will be closed via idle timeout.

\end{description}

\subsection{QuicStream Class}

The \QuicStream{} class represents a single stream in a QUIC connection and derives from the
abstract \class{Stream} class. The \class{Stream} class is a bidirectional stream abstraction and
since not all QUIC streams are bidirectional, user should check if the specific \QuicStream{}
instance supports supports the operation by inspecting the \method{CanRead} and \method{CanWrite}
properties. Invoking read methods on write-only (i.e. unidirectional sending) stream will cause an
exception to be thrown and vice versa.

The list below mentions the members specific for the \class{QuicStream} class and some important
members inherited from the \class{Stream} class.

\begin{description}

    \ditemproperty{long}{StreamId}{\propget} The Stream ID\@.

    \ditemproperty{bool}{CanRead}{\propget} Returns \keyword{true} if the stream supports reading.

    \ditemproperty{bool}{CanWrite}{\propget} Returns \keyword{true} if the stream supports reading.

    \ditemmethod{void}{AbortRead}{long} Aborts the receiving part of the stream with the provided error code.

    \ditemmethod{void}{AbortWrite}{long} Aborts the sending part of the stream with the provided error code.

    \ditemmethod{int}{Read}{Span<byte>} Reads the content
    of the stream into the provided buffer, blocks if no data is available. Returns 0 only when there will be no more data in the stream.

    \ditemmethod{ValueTask<int>}{ReadAsync}{Memory<byte>, CancellationToken} Reads the content
    of the stream into provided buffer, blocks until some data is available. Returns 0 only when there will be no more data in the stream.

    \ditemmethod{void}{Write}{Span<byte>} Writes the content
    of the provided buffer into the stream, returns when the data have been buffered internally.

    \ditemmethodWithComment{ValueTask}{WriteAsync}{*, CancellationToken}{multiple overloads} Multiple overloads of this method offer writing from various types of buffers: \class{ReadOnlyMemory<byte>}, \class{ReadOnlySequence<byte>}, and \class{ReadOnlyMemory\allowbreak<ReadOnlyMemory<byte>>}. The last one can be used to perform scatter/gather IO. The returned task completes when the provided data have been buffered internally and the buffers can be reused for other purposes.

    \ditemmethodWithComment{ValueTask}{WriteAsync}{*, bool, CancellationToken}{multiple overloads} Like the methods above, but also allow specifying that the provided data are the last on the stream and that the stream should be gracefully closed.

    \ditemmethod{ValueTask}{ShutdownWriteCompleted}{CancellationToken} The returned task completes when the stream shutdown completes. Meaning that acknowledgement from the peer is received.

    \ditemmethod{ValueTask}{Shutdown}{} Closes the stream with error code 0. And blocks until shutdown completes.

\end{description}

\subsection{Exceptions}

The QUIC API can throw following exceptions:

\begin{description}

    \ditem{\ditemsrcsize\class{QuicException}} Base class for all thrown exceptions, used when a more specific exception is not available

    \ditem{\ditemsrcsize\class{QuicConnectionAbortedException}} Thrown when the connection is forcibly closed either by the transport or by the remote endpoint.

    \ditem{\ditemsrcsize\class{QuicStreamAbortedException}} Thrown when the stream was aborted by the remote endpoint.

    \ditem{\ditemsrcsize\class{QuicOperationAbortedException}} Thrown when the pending operation was aborted by the local endpoint.

\end{description}

\section{High-Level Architecture}

The \QuicConnection{} implementation will require some sort of background processing
thread\footnote{To reduce verbosity, this text will be using term \textit{thread} to mean both code
  executing on a dedicated \class{Thread} instance or code running on the thread-pool thread using
  the \class{Task} abstraction and the \dotnet{} async/await model.} to be able to send
acknowledgements for incoming packets and resending data after being determined lost due to a
timeout. Because the correctness of a multithreaded code is hard to test, our implementation of
\QuicConnection{} will not interface with the underlying \Socket{} instance directly. Instead,
the implementation will provide an internal interace for exchanging the datagrams to be
sent/received. The actual sending of these datagrams will be handled by a separate class we named
\QuicSocketContext{}. By separating the socket I/O from the connection logic implementation, we can
write unit tests that inspect the contents of the sent datagrams and assert that their contents
conform to the protocol specification.

On the server's side, the \QuicSocketContext{} class will also handle any stateless packet
processing, such as sending Retry and Version Negotiation packet. It will be also responsible for
matching packets to the appropriate \QuicConnection{} instances and queueing new connections to \QuicListener{} to be read by the application. \autoref{fig:03-architecture} illustrates the relationship between \QuicSocketContext{}, \QuicConnection{} and \QuicListener{} classes.

\begin{myFigure}{fig:03-architecture}{High-level background processing architecture.}

  \input{img/03-architecture.pdf_tex}

\end{myFigure}

\subsection{Servicing the Socket}

The \QuicSocketContext{} class will implement the necessary background processing management. Its
responsibilies are:

\begin{itemize}

  \item routing incoming datagrams from \Socket{} to the proper \QuicConnection{}
  instance;

  \item calling timeout handlers after a timeout set by the connection expires; and

  \item sending outgoing datagrams provided by the \QuicConnection{}.

\end{itemize}

For performance reasons, it may be better to process timouts and sending of the datagrams on one
thread, and processing of received datagrams on another thread. However, the cost of synchronization
of the shared state may outweigh the performance gain. Our prototype implementation will use a
single thread to service both types of events, but will allow for the possibility of future
experimentation with separate threads for sending and receiving.

\subsubsection{Processing Multiple Connections in Parallel}

For client connections, the background processing provided by the \QuicSocketContext{} class needs
to service only a single connection. However, on the server's side, there can be multiple
connections receiving datagrams from the same socket, and therefore being served by the
\QuicSocketContext{} instance. Servicing multiple \QuicConnection{} instances by only one thread
could limit the server's throughput.

To allow parallel processing of multiple QUIC connections on serers, each \QuicConnection{} should
have a dedicated thread for background processing. Our implementation separates the per-connection
logic into separate \QuicConnectionContext{} class. The \QuicSocketContext{} class will be,
therefore, directly responsible only for stateless packet processing like sending Version
Negotiation for incoming packets with unsupported versions. Packets belonging to existing
connections will be quequed for processing by the appropriate \QuicConnectionContext{} instance. A
possible runtime structure of the \QuicSocketContext{} servicing two separate connections is
illustrated in \autoref{fig:03-socket-context-architecture}. The two \QuicConnectionContext{}
instances are serviced by separate dedicated threads. The \QuicSocketContext{} itself does not need
a dedicated thread. Instead, its logic is performed in whatever thread that completes the pending
asynchronous socket receive call.

\begin{myFigure}{fig:03-socket-context-architecture}{Architecture of the server-side background processing.}

\resizebox{\linewidth}{!}{\input{img/03-server-socket-context.pdf_tex}}

\end{myFigure}

\subsubsection{Accessing the Socket Object}

Introduction of \QuicConnectionContext{} solved possible performance bottlenecks for servers, but it
introduces a complication in the \Socket{} access. Ideally, Each thread would call the
\method{ReceiveFromAsync} method on the \Socket{} with the known remote endpoint address. This
naive approach would work, considering that we plan to omit support for connection migration.
However, as mentioned in \autoref{sec:02-path-validation}, the endpoint can change its address also
due to NAT rebinding. In that case, the connection would be eventually terminated due to the idle
timeout (see \autoref{sec:02-idle-timeout}).

Therefore, our implementation will use a separate thread to receive all UDP datagrams, and queue
them into a \class{Channel<>} instance of the appropriate \QuicConnectionContext{} based on the
value of the DCID field of the packet header.

\subsubsection{Future Support for Connection Migration}

Lastly, we need to review the architecture for possible support for connection migration. Thanks to
the \QuicConnectionContext{} not depending directly on a particular \Socket{} instance, it is
conceiveable that a single \QuicConnectionContext{} instance could be part of two
\QuicSocketContext{}s --- one for the old endpoint address, and the other for the new one. Use of the
\class{Channel<>} class already ensures that the incoming datagrams could be queued concurrently by
both \QuicSocketContext{} instances. The only modification needed on the architecture level would be
allowing the \QuicConnectionContext{} to select the right \Socket{} from which the outgoing
datagrams should be sent.

\subsection{Public API Threading model}

The target API uses the \class{ValueTask} type designed for efficient asynchronous methods. Using
this type, user code will start an asynchronous operation, that can be completed by a background
thread servicing the connection. This way the user code cannot block the execution of the
connection's background thread which could otherwise cause timeouts to be missed.

The implementation should make possible to use the \QuicConnection{} from multiple threads when
it makes sense. For example, it can be useful to be able to process individual QUIC streams in
parallel. Therefore, when designing the implementation we will assume following threading model for
the API:

\begin{itemize}

  \item Individual streams can be used concurrently. However, a single stream can be
    accessed only by one thread at a time.

  \item Accepting/opening new streams on a connection can be done concurrently from multiple threads.

  \item All other operations on \QuicConnection{} must be synchronized. This includes, e.g.,
    starting and aborting the connection.

\end{itemize}

\section{Packet Serialization/Deserialization}

Special care needs to be taken when implementing serialization of the QUIC packets to the wire
format because inefficient implementation can have very negative impact on the overall performance.
Because significant amount of time is spent parsing and processing individual QUIC frames. The
packet and frame representation should be carefully designed to avoid allocations on the hot path.

The incoming packets can contain arbitrary encoding errors which should be handled without the use
of exceptions for above-mentioned performance reasons. Therefore, all possible errors encountered
during packet deserialization must be handled gracefully. This includes values being outside of
range of allowed values and incomplete or damaged packets.

\subsection{QUIC Packet and Frame Representation}

Some packets contain a large number of fields and, therefore, passing them around as individual
variables would make the implementation harder to maintain. Logically, the QUIC frames form coherent
messages that should be represented by individual \dotnet{} types. This gives us also an opportunity to keep the serialization and deserialization code next to each other, making it easier to ensure that e.g. the order of the serialized fields match in both methods.

Representing QUIC frames as instances of individual classes would introduce a lot of heap
allocations for every received QUIC packet. This thesis therefore models QUIC frames headers of QUIC
packets as value types. Also, payload of some frames may consist of large blocks of memory. Examples
include \STREAM{} and \CRYPTO{} frames, which can fill the entire payload of the QUIC packet.
Duplicating this block of memory would be another unnecessary memory allocation. \dotnet{}~2.1
introduced the \class{Span<T>} type which can be used to efficiently reference arbitrary memory
block, including memory allocated on stack using the \keyword{stackalloc} keyword.

However, the \class{Span<T>} type is a \keyword{ref struct} --- a special kind of value type which can
be stored only in local variables or inside other \keyword{ref struct}s. Limitations on usage of
\keyword{ref struct}s also forbid their use as generic type arguments, e.g. for \class{Func<>} and
other generic delegates. Although this limitation does not pose problems for the \QuicConnection{}
implementation which can process the frames immediately as they are parsed, it could complicate
writing of unit tests.

Because it is impossible to create a \class{List<>} containing all frames from a particular QUIC
packet or an \class{Action<>} delegate from a lambda inspecting the contents of a specific QUIC
packet or frame, the code for writing unit tests would have to seek inside the serialized frame
payload, which would be very verbose. Additionally, it would be hard to modify certain kinds of
frames in order to simulate error conditions because such modifications could change the serialized
length of the frame and thus all following frames would have to be moved.

In order to keep some flexibility for writing unit test code, this thesis will duplicate the types
for frame representation using \keyword{class}es. This duplication will allow expressing the unit
tests in a succint, declarative manner, as demonstrated in \autoref{lst:03-desired-unit-test} and
\autoref{lst:03-unit-test-intercept}.

\begin{myListing}{lst:03-desired-unit-test}{Example unit test inspecting QUIC packets contents.}{Fact, Assert, Datagram, InitialPacket, CryptoFrame}{}
    [Fact]
    public void ClientInitialDatagramHasInitialPacketWithCryptoFrame()
    {
        var datagram = GetDatagramToSend(Client);
        Assert.Equal(QuicConstants.MinimumClientInitialDatagramSize, datagram.Size);

        var initial = Assert.IsType<InitialPacket>(Assert.Single(datagram.Packets));
        Assert.Equal(0u, initial.PacketNumber);
        initial.ShouldHaveFrame<CryptoFrame>(crypto =>
        {
            Assert.Equal(0, crypto.Offset);
            Assert.NotEmpty(crypto.CryptoData);
        });
    }
\end{myListing}

\begin{myListing}{lst:03-unit-test-intercept}{Example unit test simulating error conditions.}{Fact, Assert, InitialPacket, QuicError}{TransportErrorCode}
    [Fact]
    public void SendsConnectionCloseWhenReservedBitsAreSet()
    {
        InterceptFlight(Client, Server, flight =>
        {
            // simulate sending wrong value in reserved bits
            InitialPacket initial = Assert.IsType<InitialPacket>(flight.Packets[0]);
            initial.ReservedBits = 0x01;
        });

        GetInitialToSend(Server)
            .ShouldHaveConnectionCloseFrame(
                TransportErrorCode.ProtocolViolation,
                QuicError.InvalidReservedBits);
    }
\end{myListing}

\subsection{QuicReader and QuicWriter}

Both serialization and deserialization requires maintaining the current position in the buffer
containing the memory to deserialized. To simplify this, our implementation introduces \QuicReader{}
and \QuicWriter{} classes as a primary means to reading and writing QUIC primitives to memory.
Although \QuicReader{} and \QuicWriter{} are reference types allocated on the heap, their instances
are expected to be cached by the class that uses them.

The \QuicReader{} and \QuicWriter{} classes also take care of converting the endianity of the data
if the application runs on little-endian platform, because QUIC uses network order (big-endian). This is done by forwarding the calls to respective methods on the \class{BinaryPrimitives} class.

\section{Stream Implementation}

QUIC is a transport protocol and therefore its entire purpose is transferring streams of data. Since
it is likely to be the hot path of the implementation, the internal handling of streams must be
efficient and avoid unnecessary copying of blocks of stream data.

QUIC recognizes four types of streams. These streams can have sending part, receiving part, or both.
The fact which endpoint initiated the stream controls only which flow control limits apply to that
stream. Otherwise all streams are handled equally. As mentioned in \autoref{sec:02-stream-types},
bidirectional streams can be implemented as two unidirectional streams. Therefore, our
implementation will divide the QUIC stream logic into \SendStream{} and \ReceiveStream{} classes,
which will handle sending and receiving behavior of the stream.

\subsection{Receiving Part of the Stream}

The implementation of \ReceiveStream{} stream must be able to buffer the received data in case the
QUIC packets send by the peer were lost or reordered. It also needs to track the amount of buffered
memory and how much data was delivered to the application in order to correctly update flow control
limits for the peer.

\subsubsection{Stream Data Buffering}

Ideally, the stream implementation would be structured in a way that the stream data were copied
straight from the decrypted packet to the memory provided by the application. This would be possible
if the API used an event-based model with callbacks for incoming data. The API, as currently
designed, utilizes method-based model. If the application does not call the \method{ReadAsync}
method, there is no buffer to deliver to.

In order to avoid additional copies, the buffer holding the stream data cannot be reused to receive
another QUIC packet, because the part of the buffer with the \STREAM{} frame must be kept unmodified
until delivered to the application. Instead, different buffer must be obtained for receiving the
next packet. For such a solution to be memory efficient, it would require a complex memory pooling
scheme to avoid needlessly, essentially implementing a custom memory allocator over a block of
memory. The performance improvement of such a solution may be greatly outweighed by the development
and maintenance costs. Therefore, until the cost of such a solution is justified by performance
measurements, our implementation will use a two-copy approach: the first copy from the packet to an
intermediate buffer, the second copy from the intermediate buffer to the destination memory provided
by the application.


\todo{if we speak about doing some performance measurements, then they should be done and their
  results presented in some following chapter}

\todo{forward reference to the measurements, see 2020-10-22: 40m}

\subsubsection{Packet Reordering and Data Deduplication}

Unreliability of the UDP protocol can cause QUIC packets to be reordered or lost. Because of that,
parts of the stream may be received multiple times, and the contents of \STREAM{} frames can
arbitrarily overlap.

Even though QUIC Flow control provides an upper limit on the data that needs to be buffered at any
given moment, allocating one large buffer may be inefficient, as the entire buffer might not be
needed at any given point in time. Our implementation therefore uses multiple smaller buffers and
allocates only the necessary number of buffers that are needed to buffer currently received data. To
reduce pressure on garbage collector, buffers that are no longer necessary are returned to a shared
instance of \class{ArrayPool<byte>} class to avoid their frequent allocation.

\subsubsection{Reading Data by Application Code}

Because user code runs on a different thread from the internal \QuicConnection{} logic, there needs
to be some synchronization involved. Our implementation uses the \class{Channel<>} class, which
provides an efficient implementation of producer-consumer queue with support for asynchronous
operations using the \class{ValueTask<>} type. Each time an incoming \STREAM{} frame allows a part
of the stream to be delivered to the application, the relevant region of the buffer is queued into
the \class{Channel<>} using the \class{Memory<byte>} type.

\todo{more of the descriptions of the}

\todo{the image is hard to understand, it needs a more detailed description and a legend in the image so that the reader does not have to}

\todo{consider organizing the layers vertically in the image}

\todo{even though I explain the buffering etc earlier, I need to describe the image}

\autoref{fig:03-receive-stream} Illustrates how all parts of the \ReceiveStream{} fit together.
From the left, data from the incoming \STREAM{} frame are temporarily stored in pooled reordering
buffers. Regions of memory containing deliverable data are then queued for delivery in the channel.
From the channel, the memory regions are retrieved by the application thread and copied into the
destination buffer provided by the application.

\begin{myFigure}{fig:03-receive-stream}{Implementation of the receiving part of the stream}

  \resizebox{\linewidth}{!}{\input{img/03-receive-stream.pdf_tex}}

\end{myFigure}

\subsubsection{Flow Control Considerations}

QUIC Flow control limits for streams specify the maximum offset of data that an endpoint can send.
An endpoint needs to update these limits by sending a \MAXSTREAMDATA{} frame after the data are
delivered to the application. However, sending updated limits in every packet potentially wastes
space that could be used by the application data. On the other hand, updating the limits too late
could lead to the other endpoint being blocked and signal the fact using the \STREAMDATABLOCKED{}
frame. In such state, the endpoint is unable to send more application data until it receives an
update via \MAXSTREAMDATA{} frame.

\todo{check this}

Our implementation will send a \MAXSTREAMDATA{} frame after the client uses more than half of the
limit provided since the last update. This should prevent updates being too often, and at
the same time early enough that the sender does not run out of flow control credit.

\subsection{Sending Part of the Stream}

The sending part of a QUIC stream must keep track of state of all outbound data on the stream. Any
part of the stream can be lost during transmission and the has to be retransmitted, and any part of
the sent data can be acknowledged.

\subsubsection{Stream Data Buffering}

Similarly to buffering received data, we will analyze how many times the data need to be copied
before they are sent. The semantics of the \method{Write} and \method{WriteAsync} methods is that
when the method completes, the provided memory can be reused. \todo{see question from PJ on
  2020-10-22 1h about multiple parallel ValueTasks and completing when the data are acknowledged} A
single-copy approach in this case would require that the \method{Write} and \method{WriteAsync}
methods complete only after the peer acknowledges that the contained data were received. In the
best-case scenario that would take a full round-trip which would be very inefficient. Therefore our
implementation will, as with the receiving part of the stream, use an intermediate buffer for
storing the data.

Our implementation will also use variable number of fixed-size buffers rented from a shared
\class{ArrayPool<byte>} instance. These buffers will be used to store data that cannot be discarded
yet, because the other endpoint did not acknowledge their reception.

\subsubsection{Acknowledgement, Loss and Retransmission}

Each byte that was written into the stream can be conceptually in three different states:

\begin{itemize}

  \litem{Pending} The byte needs to be sent to the peer in some future \STREAM{} frame.

  \litem{In-flight} The byte has been sent, but it is uncertain if the containing packet was
received.

  \litem{Acknowledged} The packet containing the byte has been acknowledged by the other endpoint. It is no longer necessary to buffer this byte.

\end{itemize}

The transitions between these states are straightforward. The data transition from pending to
in-flight by sending them via \STREAM{} frame. When the packet is deemed lost by the loss detection
algorithm, the data transition back to pending, and if the packet has been acknowledged, the data
transition to acknowledged. The state of the individual parts of the stream can be tracked by
maintaining three sets of \textit{ranges} --- starts and ends of the blocks of data in the same state.

\subsubsection{Writing Data by Application Code}

Our implementation will, again, use \class{Channel<>} type to provide synchronization with the
application thread. However, since \class{Channel<>} does not allow random access to the provided
data, a separate list of buffers with yet unacknowledged data need to be maintained as well.

\autoref{fig:03-send-stream} illustrates the process of writing data into the \SendStream{}. The
data from the application are coming from the left, where they are written into a buffer which is
queued in the synchronization channel. From the channel, the buffer is dequeued and placed in a list
of retransmission buffers, where it stays until all data it contained are ackonwledged by the other
endpoint.

\todo{why can't I gather-send from the buffers? because I need to encrypt, potentially more than
  once if the data gets lost}

\begin{myFigure}{fig:03-send-stream}{Implementation of the sending part of the stream}

  \resizebox{\linewidth}{!}{\input{img/03-send-stream.pdf_tex}}

\end{myFigure}


\subsection{Abort/Dispose Model for Streams}

The \QuicStream{} class implements the \interface{IDisposable} interface which should provide
automatic closing of the stream when \QuicStream{} is used in a \keyword{using} statement or
\keyword{using} block. This implies resetting writable part of the stream using \RESETSTREAM{} frame
and requesting reset of the readable part of the stream using \STOPSENDING{} frame.

However, both these frames require an application-level error code. QUIC specification does not
define any transport-level error codes for these stream operations. This has been established as an
incorrect design and can be expected to change in the future iterations of the API design.

Leaving \interface{IDisposable} unimplemented would be misleading to users in the prototype
implementation. Our implementation will, therefore, use error code 0 regardless of it's semantics in
the application protocol, unless the stream has been explicitly closed by the application using the
\method{AbortRead} or \method{AbortWrite} methods.

Another area where the QUIC API is not well defined is the behavior of pending \method{AcceptStream}
call on \QuicConnection{} when the connection is closed. In case the connection has been closed
gracefully from the application protocol's perspective, it would be better if this method didn't
throw an exception, but returned \keyword{null} reference. However, the semantics of the error codes
are, again, defined by the application protocol. Until the behavior of these methods in such
scenarios is better defined, our implementation will throw
\class{QuicConnectionAborted\allowbreak{}Exception} for all pending async calls.

\section{TLS Implementation}

TLS handshake forms an integral part of the QUIC connection establishment. Because correct TLS
implementation is crucial for ensuring the security of the resulting implementation, this thesis
should avoid implementing TLS algorithms by itself. Instead, it should reuse some existing and
well-tested implementation.

The novel way in which QUIC integrates with TLS requires specific functionality on the API of the
TLS implementation. Below is a nonexhaustive list of operations the TLS library's API must provide:

\begin{itemize}

  \item Current state of the handshake

  \item Temporary keys used to protect the handshake process

  \item Raw unencrypted TLS messages to be sent to the other endpoint

  \item Negotiated cipher

  \item Specifying protocols used for ALPN

  \item Specifying custom TLS extension to exchange QUIC transport parameters

\end{itemize}

The \dotnet{} runtime libraries use different native libraries to provide TLS functionality on
different operating systems. On Windows, \libname{Secure Channel}~\cite{Schannel} (\libschannel{}
for short), which is part of the Windows operating system. On Linux and macOS systems, the
\libopenssl{} library~\cite{OpenSSLWeb} is used.

\begin{description}

    \ditem{\libname{Secure Channel}}
    The \libschannel{} versions present in the latest Windows 10
    builds support only TLS 1.2.  However, future updates will implement also TLS 1.3. The
    \libschannel{} version with TLS 1.3 support can be obtained by installing an insider preview
    build of Windows 10.

    The \libmsquic{} library uses \libschannel{} library when compiled for Windows. Therefore, we
    assume that \libschannel{} exposes the necessary API for our managed QUIC implementation.

    \ditem{\libopenssl{}}
    None of the released versions of \libopenssl{} library expose necessary API for integration
    with QUIC, and there are no plans to include such API in the next \libopenssl{} 3.0.0
    release~\cite{OpensslBlogNoQuic}.

    However, developers at Akamai maintain a fork of \libopenssl{} which adds the QUIC-enabling
    API~\cite{AkamaiOpensslGithub}. This modified version of \libopenssl{} is used by \libmsquic{}
    (on Linux) and some other QUIC libraries like \libname{quiche} from
    Cloudflare~\cite{quicheGithub}. It is possible that the changes made by Akamai will be merged
    into \libopenssl{} into the following 3.1.0, or later releases.

\end{description}

In conclusion, the APIs required for our QUIC implementation are currently only accessible in only
the preview versions of Windows 10. Relying solely on \libschannel{} for TLS 1.3 support in our
prototype implementation would severely impact cross-platform availability.

The \libopenssl{} library and supports all platforms supported by \dotnet{}, and could therefore be
used to implement TLS integration into QUIC in a platform-compatible manner. This solution, however,
has some drawbacks:

\begin{itemize}

  \item Modified \libopenssl{} binary must be distributed with \dotnet{} runtime.

  \item Only limited integration with X.509 certificates is possible because the
    \class{X509Certificate} class implementation will be using different binary ---
    \libname{CryptoAPI} on Windows, unmodified \libopenssl{} on Linux and macOS.

\end{itemize}

These drawbacks are acceptable for the prototype implementation, and will be eliminated once the
modified \libopenssl{} is replaced with \libschannel{} integration for Windows, and mainstream
version of \libopenssl{} once the support for QUIC is released.

This thesis will therefore integrate with the forked \libopenssl{} library from Akamai. Because the
library is written in C, it has to be integrated to the build process to be build together with the
native code of the \dotnet{} runtime.

\section{Packet Protection}

As described in \autoref{sec:02-packet-protection}, the packet encryption consists of two phases ---
payload protection and header protection. The combined process requires following inputs:

\begin{itemize}

  \item Keys derived from the protection secrets (as explained in
  \autoref{sec:02-encryption-key-derivation})

  \item Negotiated cipher

  \item Packet number (for encryption), or expected packet number (for decryption)

  \item The QUIC packet to encrypt or decrypt

\end{itemize}

The cipher is negotiated once and cannot be changed during the lifetime of the connection. Keys for
the protection can be changed only for the 1-RTT packets using the process of \textit{key update}
(see \autoref{sec:02-key-update}), which can be expected to be relatively infrequent. The rest of
the inputs change with every packet. Therefore, our implementation encapsulates the packet
protection implementation in a \class{CryptoSeal} class, which does not depend on the rest of the
QUIC implementation.

The process of receiving packets requires intermediate validation of the header fields. The
individual steps --- header protection and payload protection --- must be therefore exposed separately.
Also, the protection should happen in-place to avoid unnecessary allocations and copying of the
packets.

An important consideration needs to be made for performing the actual encryption/decryption once all
inputs to the AEAD have been gathered. \dotnet{} does not contain an implementation of the CHACHA
family of ciphers. Because the implementation can work without it by instructing TLS library to not
allow its negotiation, the prototype implementation of QUIC will not support this kind of cipher.
The other ciphers are based on the AES family of ciphers, which are supported by \dotnet{}, but
individual classes implementing these ciphers do not share a common interface. Our implementation
therefore wraps the concrete AES implementations in classes derived from an abstract
\class{CryptoSealAlgorithm} class defining a common interface required by \CryptoSeal{}.
\autoref{fig:03-crypto-seal} illustrates how the \CryptoSeal{} and \class{CryptoSealAlgorithm}
classes are connected.

\begin{myFigure}{fig:03-crypto-seal}{Relationship between CryptoSeal and CryptoSealAlgorithm classes}

  \resizebox{\linewidth}{!}{\input{img/03-crypto-seal.pdf_tex}}

\end{myFigure}

Key update can be implemented by replacing the existing instance of the
\CryptoSeal{} class with the new one with the updated protection secret.

\section{Loss Detection and Loss Recovery}

\todo{This section is new}

In order to be able to detect loss packets and retransmit any lost data, the \QuicConnection{}
implementation must keep track of which data have been sent in which packets and when the packet was
sent. Our implementation will encapsulate this information together with the loss detection
algorithm implementation in a dedicated \RecoveryController{} class.

\subsection{Detecting lost packets}
\subsection{Detecting duplicate packets}
\subsection{Tail loss probe}
\subsection{Congestion window}
\subsection{Interface for Congestion control algorithm}

\section{Debuggability}

QUIC is a very complex protocol and investigation of some issues may be very difficult. By stopping
the application on a breakpoint, the developer inadvertently changes the behavior of the
implementation, which makes interactive debugging of issues which span multiple round trips almost
impossible.

There are two possible ways of gaining better insight into the behavior of the connection ---
externally inspecting the packets sent over the network, and producing verbose logs (called traces)
by the implementation.

\subsection{Inspecting the Sent Packets}

Inspecting the connection behavior by observing the packets sent over the network is a non-invasive
way of diagnosing issues in any networking protocol. An example of a tool which can be used for this
task is Wireshark \cite{web:wireshark} which also supports QUIC.

However, since QUIC is always encrypted, inspecting QUIC packets via Wireshark is not
straightforward as for other network protocols. Implementations must leak the encryption secrets
used in the connection, e.g., in a log file, and these keys must be provided to Wireshark in order
to successfully decrypt the packets.

\subsection{Producing Traces from the Connection}

A better insight into the behavior of the connection can be gained by emitting verbose logs which
can be later analyzed, but such logs may be difficult to read and reason about. Fortunately, there
are tools which can visualize individual events in the connection in a graphic format which is easy
to understand. One of these tool is \textit{qvis} \cite{web:qvis} which is part of \textit{quiclog}
suite \cite{githubquiclog}. The \textit{qvis} tool consumes logs in a JSON format, which can be
either produced directly, or generated from implementation-specific log format or even from network
packet captures created by Wireshark, provided that encryption secrets are logged elsewhere for the
given connection.

Although JSON format produces larger log files, it can be directly consumed by the \textit{qvis}
tool without any further conversions. For this reason, we chose to use it in our implementation and
will consider more efficient logging options if producing the JSON format incurs too significant
overhead.

\section{Integration into .NET Runtime Codebase}

The work of this thesis aims to be eventually mergeable into the \dotnet{} runtime codebase. As of
writing this thesis, there already exists project for \textit{System.Net.Quic.dll}. Therefore, the
source code may be placed there directly without additional changes.

There is, however, the issue with the \libopenssl{} dependency. Although it would be possible to
build the required \libopenssl{} version together with other native parts of the \dotnet{} runtime,
it would introduce multiple additional requirements on the installed tools. In order not to
complicate the runtime build requirements, our implementation will expect the appropriate
\libopenssl{} version to already present and available for loading.

It can be useful to be able to use the QUIC implementation without the need of \libopenssl{}
dependency. Because the \libopenssl{} implementation is used only for performing the TLS handshake,
it is not difficult to replace the TLS implementation by a mock implementation that will work
sufficiently when it is used on both sides of the connection. The mock TLS implementation allows the
\libopenssl{} dependency to be completely optional, and is required only when testing interop with
other QUIC implementations. Therefore, our implementation will first inspect if required
\libopenssl{} version is available, and if it is not, then it will fallback to mock TLS
implementation.
