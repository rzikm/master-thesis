\chapter{Evaluation}\label{chap:04-evaluation}

This chapter evaluates the performance of the managed QUIC implementation developed in this thesis
and compares it to the existing \libmsquic{}-based implementation. We will also compare both managed
and \libmsquic{}-based QUIC implementations to \class{SslStream} which uses a combination of TCP and
TLS\@. In our evaluation, we will focus on two performance characteristics relevant for network
communication:

\begin{itemize}

        \litem{Throughput} The rate at which the application data are sent. High total throughput is
essential for servers when handling a large number of parallel connections.

        \litem{Round Trip Time Latency} The delay between sending a request and receiving a
response. This metric is important mainly for clients, where lower latencies mean faster responses
from the server. Rather than measuring the average latency, it is more common to measure, e.g., the
99th percentile of latency, which indicates minimum expected latencies in the slowest 1\% replies.
This metric is vital in the context of webpages and communication with browsers, especially for
websites for which web browsers generate a large number (even hundreds) of HTTP requests for a
single page access. In such cases, the probability of at least one of the requests being slower than
the 99th percentile raises significantly and, therefore, a high value of the 99th percentile would
negatively affect the total page load time for a significant number of users~\cite{Treat2015}.

\end{itemize}

\section{Evaluation Environment}

We have evaluated our implementation in two different environments. Our primary evaluation
environment consists of two blade servers on the same LAN network. For short, we will reffer to this
environment as the \textit{Linux LAN} environment. The relevant software and hardware parameters of
the servers are:

\begin{itemize}

        \litem{CPU} Intel\textsuperscript{\textregistered{}} Xeon\textsuperscript{\textregistered{}}
E3-1270 v6 @ \SI{3.80}{\giga\hertz} (\SI{4.20}{\giga\hertz} turbo boost, 4 cores, 2 threads per
core)

        \litem{RAM} \SI{32}{\giga\byte} (\SI{2400}{\mega\hertz})

        \litem{Network Interface Card} Intel\textsuperscript{\textregistered{}} Ethernet Controller
I350 (\SI[per-mode=symbol]{1}{\giga\bit\per\second})

        \litem{OS} Fedora 32

\end{itemize}

Unfortunately, the two servers are connected only by \SI[per-mode=symbol]{1}{\giga\bit\per\second}
Ethernet, which implies a theoretical upper limit for TCP's throughput at
\SI[per-mode=symbol]{117.5}{\mega\byte\per\second} and similar limit for UDP\@. In some
measurements, we reached values that approached this limit. Therefore, when appropriate, we also
evaluated our implementation using a loopback network interface on a desktop workstation PC\@. For
short, we will reffer to this environment as \textit{Win loopback}. The machine has following
specifications:

\begin{itemize}

        \litem{CPU} Intel\textsuperscript{\textregistered{}} Core\textsuperscript{\textregistered{}}
i7-8700 @ \SI{3.20}{\giga\hertz} (\SI{4.60}{\giga\hertz} turbo boost, 6 cores, 2 threads per core)

        \litem{RAM} \SI{32}{\giga\byte} (\SI{2400}{\mega\hertz})

        \litem{OS} Windows 10 version 20H2\footnote{Update to Windows 10 20H2 includes significant
TCP and UDP performance improvements~\cite{theregister20h2}. Measurements taken on older versions of
Windows would yield significantly different results.}

\end{itemize}

In both environments, the testing application was run in a a 64-bit process on preview
\dotnet{}~6.0.0 host (CoreCLR 6.0.20.55508, CoreFX 6.0.20.255508).

\subsection{Drawbacks of Measuring Loopback Performance}\label{sec:04-localhost-mtu}

Measuring over the loopback interface removes any upper limit on bandwidth because the loopback
interface is implemented purely in software inside the operating system. It also minimizes the
transport latency and guarantees zero packet loss during transport. The software-based
implementation of the loopback interface allows optimizations that are not feasible on Ethernet
networks. As an example, on Windows operating system, loopback connections can send
\SI{64}{\kibi\byte} IP packets --- which is the maximum size allowed by the IP protocol --- without any
link-layer fragmentation. On the other hand, Ethernet connections can carry only up to
\SI{1500}{\byte} IP datagrams without fragmentation. The greater size of datagrams on the loopback
interface reduces overhead when TCP is used for inter-process communication on the same machine.

Both managed and \libmsquic{}-based QUIC implementations send IP datagrams which are at most
\SI{1500}{\byte} even on loopback connections, because of a hardcoded upper limit on UDP datagram
size in both implementations\footnote{QUIC does not impose an upper limit on the UDP datagram size.
However, it requires that the UDP datagrams be small enough to avoid fragmentation. The hardcoded
limit has been chosen to simplify the implementation and to work well on Ethernet connections. This
limit may be removed in future versions of the libraries}. In order to make the loopback connection
measurements more comparable to those done on Ethernet connections, we leveraged the ability to limit
the maximum outbound IP packet size to \SI{1500}{\byte} on Windows sockets API for all loopback TCP
connections in our tests. From \csharp{}, this can be achieved using following statement:

\lstsetcsharpsettings{}
\smallskip\begin{lstlisting}[numbers=none,enums={SocketOptionLevel,SocketOptionName}]
socket.|SetSocketOption|(SocketOptionLevel.IP,
  (SocketOptionName)76 /* IP_USER_MTU */, 1500 /* size */);
\end{lstlisting}\smallskip

However, the \texttt{IP_USER_MTU} option is only available on Windows 10 version 20H2 or newer. On
other platforms, the \method{SetSocketOption} call above would throw an exception. In order to keep
cross-platform compatibility, the relevant lines have been commented out in the attached version of
the source code. However, they was used to obtain the results presented in this chapter.

\subsection{Running with MsQuic Support}

\libmsquic{}-based QUIC implementation does not work in the default installation of \dotnet{}
because \dotnet{} does not distribute the \libmsquic{} binary. Applications that wish to use the
\libmsquic{}-based QUIC implementation should reference the
\texttt{System.Net\allowbreak{}.Experimental.MsQuic} NuGet
package~\cite{SystemNetQuicExperimentalMsquic} which provides the necessary \libmsquic{} binary.
However, the Windows \libmsquic{} binary distributed in the NuGet package requires a Windows Insider
Preview build of Windows because it depends on newer \libschannel{} version for TLS implementation,
which is not available in the mainstream Windows version.

Unfortunately, we cannot install a preview build of Windows on the workstation we will use for
running the tests. Fortunately, in order to support Linux, \libmsquic{} can also use the same
modified \libopenssl{} version for the TLS implementation as our managed QUIC implementation. Even
though building \libmsquic{} with \libopenssl{} for Windows is not officially supported, we were
able to produce such a build with small changes in the \libmsquic{} build configuration. Therefore,
the \libmsquic{} library used in experiments in this chapter has been compiled locally using source
code from commit \texttt{dc2a6cf0dd12e27} from the official \libmsquic{}
repository~\cite{msquicGithub} and configured to use the same \libopenssl{} library as the managed
QUIC implementation. It should be noted that our custom Windows build of \libmsquic{} may show
slightly different performance characteristics than the official build for Windows, which uses the
\libschannel{} library.

On Linux, we could use the above mentioned
\texttt{System.Net\allowbreak{}.Experimental.\allowbreak{}MsQuic} NuGet package, but that would
complicate the build process because the package would have to be referenced only on Linux builds.
Therefore, we have decided to compile the Linux build of \libmsquic{} locally as well. Like the
Windows build, the Linux build of \libmsquic{} used in our tests was compiled from commit
\texttt{dc2a6cf0dd12e27} but did not require changes to build configuration, as it uses the modified
\libopenssl{} library by default. Built \libmsquic{} binaries for both Windows and Linux operating
systems which were used in tests can be found in the \filename{bin/{platform}/msquic/} directory of
this thesis attachments.

All measurements of the \libmsquic{}-based QUIC support for \dotnet{} include any overhead
introduced by the interop layer bridging the native library API to \dotnet{} QUIC API\@. The
measured results, therefore, do not represent raw \libmsquic{} library performance, but the
performance observed by \dotnet{} applications which use the \libmsquic{}-based implementation
provider which can be validly compared to the performance of our managed QUIC implementation.

\subsection{Benchmarking Application}\label{sec:04-benchmark-app}

The actual throughput and latency measurements are done using a dedicated benchmarking \dotnet{}
application whose source code can be found in the
\filename{src/supplementary/benchmark/ThroughputTests/} directory in the attachments of this thesis.
The attachments also include a self-contained build of the application for both Windows and Linux
operating systems in the \filename{bin/{platform}/ThroughputTests/} directory. The self-contained
application does not require \dotnet{} to be installed on the target machine. Note that on Linux,
the \texttt{LD_LIBRARY_PATH} must be defined to the directory with the compiled binary for
\libmsquic{} to be loaded correctly.

The application implements a trivial echo server and clients which exchange messages of the
specified size. The first parameter to the application selects one of the following modes:

\begin{description}

  \ditem{\texttt{server}} Starts only the server-side part of the application.

  \ditem{\texttt{client}} Starts only the client-side part of the application. In this mode, the
application spawns multiple clients and reports the measurement results on the standard output.

  \ditem{\texttt{inproc}} Starts both the server and client in the same process. All network
communication is done over the loopback network interface.

\end{description}

The benchmarking application accepts multiple parameters. A list of the most important follows. The
full list of parameters can be found in the \filename{Program.cs} file, or by running the program
with the \texttt{--help} parameter.

\begin{description}

    \ditem{\texttt{-e, --endpoint}} The endpoint on which to listen (server) or to which to connect
(client). Not applicable for \texttt{inproc} mode.

    % \ditem{\texttt{--certificate-file}} Path to the X.509 certificate file.

    % \ditem{\texttt{--key-file}} Path to the X.509 certificate private key file.

    \ditem{\texttt{-t, --tcp}} Use TCP instead of QUIC\@.

    \ditem{\texttt{-c, --connections}} The number of connetions to create.

    \ditem{\texttt{-s, --streams}} The number of streams to create in each connection. Only for QUIC.

    \ditem{\texttt{-m, --message-size}} The size of sent messages in bytes.

    \ditem{\texttt{-w, --warmup-time}} Time before starting to take measurements. This can be used to give the application time to JIT all methods used on the hot path.

    \ditem{\texttt{-d, --test-duration}} Time after which the measurement should stop and the application exit.

    % \ditem{\texttt{-i, --reporting-interval}} Delay between reports of intermediate measurements.

    \ditem{\texttt{-n, --no-wait}} Whether clients wait for a reply before sending another message.

\end{description}

By default, the implementation uses managed QUIC implementation. Switching to \libmsquic{}-based
implementation is achieved by defining the \texttt{DOTNETQUIC_PROVIDER} environment variable to
\texttt{msquic}.

The client mode of the application switches between two behaviors. By default, clients wait for a
reply from the server and measure the delay until a reply is received. When using the \texttt{-n}
switch, clients send as many messages as possible without waiting for the server and only count the
number of replies. This lets us measure the total throughput of the system.

\section{Measurement Results}\label{sec:04-perf-results}

In the following subsections, we present the results of the individual performance experiments. The
throughput and latencies were measured as follows:

\begin{itemize}

  \litem{Throughput} Throughput of the entire server. This is the total amount of application data
echoed back to clients across all connections. The throughput measurements were taken with the use
of \texttt{-n} flag in the benchmark application.

  \litem{Latency} The delay between client writing the message to the \Stream{} and reading back the
full reply. In our tests, we will measure the 99th percentile of latency.

\end{itemize}

We will perform three kinds of performance comparisons:

\begin{itemize}

        \litem{Multiple Parallel Streams Performance} These tests compare how the managed and
\libmsquic{}-based QUIC implementations scale with increasing server load. These tests will utilize
the stream multiplexing of QUIC to send messages using multiple parallel QUIC streams.

        \litem{Single Stream Performance} These tests use only a single stream in a connection.
These tests should allow us to compare the performance between QUIC implementations and
TCP+TLS-based \SslStream{}.

        \litem{Performance in Simulated Cellular Network} In these tests, we will try to approximate
the characteristics of a real-world cellular network by increasing lag and packet loss in the Linux
LAN environment.

\end{itemize}

In all experiments in this section, client and server parts of the benchmarking application were run
in separate processes. Each test case had a \SI{5}{\second} warm-up time and collected data for
another \SI{15}{\second}. These intervals were long enough to produce stable measurements across
multiple test runs.

In the benchmarking application, clients and server exchange messages of the size specified by the
\texttt{-m} parameter. In our tests, we will use mainly two message sizes: \SI{256}{\byte} and
\SI{4096}{\byte}. The \SI{256}{\byte} message are small enough to fit into a single QUIC packet,
while the larger \SI{4096}{\byte} messages are guaranteed to be split across multiple QUIC packets.
We expect that increasing the message size should increase the latency because more packets need to
be sent for both the message and the reply. Also, increasing the message should increase throughput
because there are fewer calls to the \method{Write} and \method{WriteAsync} methods for the same
amount of data.

Running the benchmarking application for tests in this section has been automated using a
PowerShell~\cite{powershell} script. The script can be found at
\filename{src/supplementary/benchmark/ThroughputTests/run.ps1} in this thesis' attachments. Comments
at the top of the file explain the usage of the script.

\subsection{Multiple Parallel Streams Performance}\label{sec:04-multi-stream-perf}

The first set of tests compared the managed and \libmsquic{}-based QUIC implementations. These tests
were run with increasingly larger messages and with a greater number of parallel connections and
streams to see how the two implementations scale.

\autoref{fig:04-lab-multi-stream-throughput} shows the measured throughput.
\autoref{fig:04-lab-multi-stream-throughput-a} shows the baseline performance when \SI{256}{\byte}
messages are sent using a single stream per connection. The other figures show measurements after
increasing message size to \SI{4096}{\byte} (\autoref{fig:04-lab-multi-stream-throughput-b}),
increasing number of streams to 32 (\autoref{fig:04-lab-multi-stream-throughput-c}), or both
(\autoref{fig:04-lab-multi-stream-throughput-d}). An immediate observation can be made that the
managed implementation produced very similar pattern in all four test runs, with the highest
throughput at 4 parallel connections and then decreasing. On the other hand, the \libmsquic{}-based
implementation maintains almost the same throughput regardless of the number of connections, but
increases throughput both when increasing the message size and number of streams in a connection.

\begin{myFigure}{fig:04-lab-multi-stream-throughput}{Multiple stream QUIC throughput measurements (Linux LAN)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-stream-throughput-a}{1 stream, \SI{256}{\byte} messages}
\footnotesize
\input{plots/04-lab-multi-stream-throughput-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-stream-throughput-b}{1 stream, \SI{4096}{\byte} messages}
\footnotesize
\input{plots/04-lab-multi-stream-throughput-b.tex}
\end{mySubfigure}

\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-stream-throughput-c}{32 stream, \SI{256}{\byte} messages}
\footnotesize
\input{plots/04-lab-multi-stream-throughput-c.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-stream-throughput-d}{32 stream, \SI{4096}{\byte} messages}
\footnotesize
\input{plots/04-lab-multi-stream-throughput-d.tex}
\end{mySubfigure}
\end{myFigure}

In case with \SI{4096}{\byte} messages, the \libmsquic{}-based implementation consistently saturates
the \SI{1}{\giga\bit} network connection between the two blade servers. To give more perspective on
the relative performance between the two implementations, \autoref{fig:04-multi-stream-throughput}
shows results of the two test runs on the Windows workstation over the loopback interface. Depending
on the number of connections, the throughput of the \libmsquic{}-based implementation was up to four
times greater than our implementation. Results of runs with \SI{256}{\byte} messages were left out
for brevity because they did not differ from the ones in the Linux LAN environment.

\begin{myFigure}{fig:04-multi-stream-throughput}{Multiple stream QUIC throughput measurements (Win loopback)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-throughput-b}{1 stream, \SI{4096}{\byte} messages}
\footnotesize
\input{plots/04-multi-stream-throughput-b.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-throughput-d}{32 stream, \SI{4096}{\byte} messages}
\footnotesize
\input{plots/04-multi-stream-throughput-d.tex}
\end{mySubfigure}
\end{myFigure}

The peak of the throughput of our managed QUIC implementation at four connections can be explained
by the fact that we are spawning a long-running background task for each connection. Four
connections lead to a number of parallel \class{Task}s that were enough to completely utilize the
entire CPU\@.

As for the decline of the throughput of managed QUIC implementation when 16 or more connections were
used, profiling showed that a substantial amount of time was spent in the garbage collection. At 256
connections, up to 50\% of the total CPU time was spent in GC, divided into relatively large pauses
of \SI{20}{\milli\second} or more. The pauses introduced by GC led to QUIC packets being considered
lost, which led to the collapse of the congestion window in the QUIC connection, further reducing
the rate at which data were sent. This points to the fact that even though we carefully avoided
needless allocations in our implementation, there are still enough sources of allocation left to
degrade a server's performance under heavy load.

An example of a large allocation source in the managed QUIC implementation is pooling the buffers
for sending or receiving QUIC packets. Our implementation uses \ArrayPoolOf{\Byte}\texttt{.Shared}
to reuse allocated buffers. However, it does not work well with a large number of connections
because \ArrayPoolOf{\Byte}\texttt{.Shared} pools only a few \ArrayOf{\Byte} instances and lets GC
collect the rest\footnote{In \dotnet{}~5, the implementation of \ArrayPoolOf{\Byte}\texttt{.Shared}
  maintains a separate pool for each CPU core. Each per-CPU core pool organizes pooled arrays into
  17 buckets with array sizes ranging from \SI{16}{\byte} to \SI{1}{\mebi\byte}. Each bucket retains
  up to 8 array instances of similar size.}. With a large number of connections, the buffers are
rented and returned in large bursts, leading to a lot of large arrays being discarded upon return to
the \ArrayPoolOf{\Byte}\texttt{.Shared} and subsequently allocated anew.

Another source of allocations are \Socket{}\texttt{.\method{SendTo}} and
\Socket{}\texttt{.\method{ReceiveFrom}} methods which allocate of a new \class{EndPoint} instance
for each call. In order to remove this particular source of allocations, an allocation-free API for
sending or receiving UDP datagrams needs to be designed and implemented on the \Socket{} class. Such
API has already been proposed in the past~\cite{dotnetGithubZeroUdp} but has not been prioritized to
be implemented.

\libmsquic{}-based implementation shows similar throughput regardless of the number of parallel
connections. Instead, it dramatically increases with message size and only slightly with the number
of streams in the connections. When we tried sending messages larger than \SI{4096}{\byte}, the
total throughput did not increase significantly anymore. Upon closer inspection of the \libmsquic{}
network traffic using Wireshark~\cite{web:wireshark}, it turns out that in tests with
\SI{256}{\byte} messages, many of the sent packets were very small, possibly containing only \ACK{}
frames. Thus, the total available network bandwidth was mostly unutilized. When \SI{4096}{\byte}
messages are used, more space in QUIC packets is utilized, which increases the throughput. Our
investigation, however, did not uncover why the total throughput of \libmsquic{} does not increase
with the number of connections in the tests with \SI{256}{\byte} message.

\autoref{fig:04-lab-multi-stream-latency} shows the latencies measured in the Linux LAN environment
for the same four test cases as above. In all tests, the \libmsquic{}-based implementation exhibits
a lower 99th percentile of latency, especially in test cases with a high number of connections. The
same measurement on Windows loopback did not yield significantly different results.

When measuring latency, the benchmarking application waits for the server to respond before sending
another message. However, with many parallel connections and streams, the total network traffic
becomes similar to that produced when measuring throughput. This was the case, especially when we
see significant differences between the latencies of the two implementations. Therefore, the change
in latency can be attributed to the frequent and long pauses introduced by the GC that we have
described above. On the other hand, in the test runs where the managed implementation is close to
that of \libmsquic{}, the GC activity was under 5\% and did not cause long frequent pauses.

By increasing the message size, we increased the probability that the application message will be
affected by the packet loss introduced by the GC pauses. This happens because the message is spread
over multiple QUIC packets, and the loss of any of those packets will delay the entire message until
the missing part is retransmitted. This explains why increasing the message size drastically affects
the latency with a large number of parallel connections.

\begin{myFigure}{fig:04-lab-multi-stream-latency}{Multiple stream QUIC latency measurements (Linux LAN)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-stream-latency-a}{1 stream, \SI{256}{\byte} messages}
\footnotesize
\input{plots/04-lab-multi-stream-latency-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-stream-latency-b}{1 stream, \SI{4096}{\byte} messages}
\footnotesize
\input{plots/04-lab-multi-stream-latency-b.tex}
\end{mySubfigure}

\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-stream-latency-c}{32 stream, \SI{256}{\byte} messages}
\footnotesize
\input{plots/04-lab-multi-stream-latency-c.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-stream-latency-d}{32 stream, \SI{4096}{\byte} messages}
\footnotesize
\input{plots/04-lab-multi-stream-latency-d.tex}
\end{mySubfigure}
\end{myFigure}

Increasing the number of streams also increases network traffic, making our implementation
susceptible to the increased GC pauses. Also, after a certain threshold, increasing the number of
streams does not increase the traffic because the maximum throughput of the implementation has been
reached. Instead, it leads to an increase in latency because it still increases the number of
outstanding parallel requests.

Lastly, we would like to remind the reader that we measured the 99th percentile of the latency,
which considers only the slowest 1\% of requests. To give more perspective on the latency
distribution on the two implementations, \autoref{fig:04-lab-multi-stream-latency-median} shows the
median measurements of the latency. For brevity, we include only the measurements for the cases with
\SI{4096}{\byte} messages. The median measurements of latencies of our implementation are similar or
lower than that of the \libmsquic{}-based one.

\begin{myFigure}{fig:04-lab-multi-stream-latency-median}{Multiple stream QUIC latency median measurements (Linux LAN)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-stream-latency-median-a}{1 stream, \SI{4096}{\byte} messages}
\footnotesize
\input{plots/04-lab-multi-stream-latency-median-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-stream-latency-median-b}{32 stream, \SI{4096}{\byte} messages}
\footnotesize
\input{plots/04-lab-multi-stream-latency-median-b.tex}
\end{mySubfigure}
\end{myFigure}

We believe that the difference in the latency distributions between the two implementations is due
to their different architectures. \libmsquic{} spawns several worker threads (depending on the
number of CPU cores) which process events for individual connections from an event queue. The event
queue ensures that the connections are serviced evenly and at regular intervals. When the load on
the server increases, the extra latency is spread evenly across all active connections. This is
further supported by the fact that the median values for the latency of \libmsquic{} are only
slightly lower than its 99th percentile from \autoref{fig:04-lab-multi-stream-latency}.

Our implementation, on the other hand, relies solely on the \dotnet{} \class{Task} scheduler, which
schedules all background tasks independently and without any priority information. This allows for
greater variance in the latency of our QUIC implementation as replies can be delayed due to
unfortunate scheduling. In the future, our implementation's architecture should be improved to
reduce the scheduling variance and, therefore, the 99th percentile of the latency.

In summary, our QUIC implementation outperforms the \libmsquic{}-based one in scenarios where small
messages are exchanged between client and server using a small number of streams. However, our
implementation does not scale as well as the \libmsquic-based one. A large number of parallel
connections and streams increases pressure on the GC, which introduces frequent and long pauses in
the application. Lastly, our implementation's latency distribution is much less uniform than that of
the \libmsquic{}-based one because of the way background work is scheduled on the \dotnet{}
thread-pool.

\subsection{Single Stream Performance}

In the second test, we compared the single-stream performance of managed QUIC implementations to the
performance of the combination of TCP and TLS available via the \TcpClient{} and \SslStream{}
\dotnet{} classes. We will also include the measurements for \libmsquic{} for completeness.

\autoref{fig:04-lab-single-stream-throughput} shows the results of the throughput measurements taken
between the two blade servers connected by \SI{1}{\giga\bit} Ethernet. In both \SI{256}{\byte}
messages (\autoref{fig:04-lab-single-stream-throughput-a}) and \SI{4096}{\byte} messages
(\autoref{fig:04-lab-single-stream-throughput-b}) scenario, TCP implementation outperforms both QUIC
implementations. Also, out of the three implementations, our QUIC implementation is the only one
which is not bottlenecked by the \SI{1}{\giga\bit} LAN\@.

\begin{myFigure}{fig:04-lab-single-stream-throughput}{Single stream throughput measurements (Linux LAN)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-single-stream-throughput-a}{\SI{256}{\byte} messages}
\footnotesize
\input{plots/04-lab-single-stream-throughput-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-single-stream-throughput-b}{\SI{4096}{\byte} messages}
\footnotesize
\input{plots/04-lab-single-stream-throughput-b.tex}
\end{mySubfigure}
\end{myFigure}

Because the throughput of some implementations in the previous figure was limited by the
\SI{1}{\giga\bit} network, \autoref{fig:04-single-stream-throughput} shows the results measured over
the loopback interface on the Windows workstation. We can see clearly that TCP still provides
greater throughput than the \libmsquic{}-based implementation. We would like to remind the reader
that these measurements were taken with the size of the IP datagram limited to \SI{1500}{\byte} (see
\autoref{sec:04-localhost-mtu}). Otherwise TCP would send the maximum \SI{64}{\kibi\byte} packets
and would reach throughput up to \SI[per-mode=symbol]{900}{\mebi\byte\per\second}.

\begin{myFigure}{fig:04-single-stream-throughput}{Single stream throughput measurements (Win loopback)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-single-stream-throughput-a}{\SI{256}{\byte} messages}
\footnotesize
\input{plots/04-single-stream-throughput-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-single-stream-throughput-b}{\SI{4096}{\byte} messages}
\footnotesize
\input{plots/04-single-stream-throughput-b.tex}
\end{mySubfigure}
\end{myFigure}

By comparing the numbers in \autoref{fig:04-single-stream-throughput-b} to that from
\autoref{fig:04-multi-stream-throughput-d}, we see that the performance of TCP is also greater than
that of \libmsquic{}-based implementation when using 32 parallel streams per connection streams.

\autoref{fig:04-lab-single-stream-latency} shows the latency measurement results in the Linux LAN
environment. These results show that TCP exhibits substantially lower latencies than either QUIC
implementation. Very similar results were also obtained on the Windows workstation.

\begin{myFigure}{fig:04-lab-single-stream-latency}{Single stream latency measurements (Linux LAN)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-single-stream-latency-a}{\SI{256}{\byte} messages}
\footnotesize
\input{plots/04-lab-single-stream-latency-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-single-stream-latency-b}{\SI{4096}{\byte} messages}
\footnotesize
\input{plots/04-lab-single-stream-latency-b.tex}
\end{mySubfigure}
\end{myFigure}

It is worth noticing that in \autoref{fig:04-lab-single-stream-latency-a}, the latency of managed
QUIC implementation is very low until it reaches a certain threshold and then increases rapidly as a
consequence of increased GC pauses.

The results presented in this section suggest that neither QUIC implementation can compete with the
\SslStream{} in neither local LAN networks nor over the loopback interface (even with the MTU
reduction we explained in \autoref{sec:04-localhost-mtu}). It is possible, however unlikely, that
the performance over real \SI[per-mode=symbol]{10}{\giga\bit\per\second} network will yield
different results. A possible explanation for these results is that the TCP networking stack is far
better optimized than UDP\@. Also, in the Linux LAN environment, some hardware optimizations may
have been in place for TCP, such as \textit{TCP offload engine}~\cite{wiki:tcp-offload-engine} which
offloads the TCP/IP stack implementation onto the controller on the network interface card.

\subsection{Performance in Simulated Cellular Network}

In the last test, we compared the resilience of QUIC and TCP+TLS implementations in a simulated
network with delay and packet loss. When choosing the values of delay and packet loss parameters, we
tried to approximate a 4G cellular broadband network's behavior. We performed a trivial connection
speed test of local 4G network using a smartphone and the
\href{https://speedtest.net}{\texttt{speedtest.net}} testing web application. This test reported
latency of \SI{25}{\milli\second}, which we then chose as the latency in the simulated network in
our tests.

As for the packet loss in 4G network, in 2012, \citeauthor{measuring4G} have measured
characteristics of 4G networks of major US cellular network carriers and measured packet loss
percentages from 0.004\% to 0.1\%~\cite{measuring4G}. Even though the cellular network technology
may have evolved since then, we could not find more recent measurements. Therefore, we will use
those two packet loss values in our test parameters.

It should be noted that our approximation of the cellular network is not perfect. The latency in a
real-world network is not constant but has a random distribution, and the \SI{25}{\milli\second} we
measured earlier was only the mean value. Similarly, packet loss does not affect each packet
independently. Instead, packets are often lost in bursts. There are multiple models for modeling
more realistic packet loss, such as the Gilbert-Elliot model~\cite{wiki:burst-error}. However, we
were unable to find enough data to compute appropriate parameters for such models.

To simulate the 4G network, we used the \textit{Traffic Control} capabilities of the Linux kernel on
the two blade servers in the Linux LAN evaluation environment. The necessary configuration can be
set using the \texttt{tc} utility. For example, the following command issued on both machines would
simulate \SI{25}{\milli\second} lag and 0.004\% packet loss:

\begin{myVerbatim}
tc qdisc change dev eth0 root netem delay 12.5ms loss 0.004%
\end{myVerbatim}

Note that the set delay parameter is half of the desired latency because the traffic shaping done by
\texttt{tc} applies only to outbound traffic. Each endpoint, therefore, adds half of the total
latency.

\autoref{fig:04-lab-loss-throughput} shows the measured throughput for 0.004\% packet loss
(\autoref{fig:04-lab-loss-throughput-a}) and 0.1\% packet loss
(\autoref{fig:04-lab-loss-throughput-b}). As expected, The increase in packet loss leads to a
decrease in throughput. In the presence of only low packet loss, the TCP implementation exhibits an
order of magnitude greater throughput. However, the throughput of the TCP protocol decreased at a
greater rate than that of the two QUIC implementation. The throughput of the two QUIC implementation
is comparable in all tests.

\begin{myFigure}{fig:04-lab-loss-throughput}{Single stream throughput measurements in simulated network (Linux LAN)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-loss-throughput-a}{\SI{25}{\milli\second} latency, 0.004\% loss}
\footnotesize
\input{plots/04-lab-loss-throughput-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-loss-throughput-b}{\SI{25}{\milli\second} latency, 0.1\% loss}
\footnotesize
\input{plots/04-lab-loss-throughput-b.tex}
\end{mySubfigure}
\end{myFigure}

Upon closer inspection, it turns out that the managed QUIC implementation is bottlenecked by the
stream buffering implementation. In \autoref{sec:03-send-stream}, we explained the design behind the
sending part of the stream, namely the \SendStream{} class. The implementation uses up to eight
\SI{16}{\kibi\byte} buffers to store the application data. Once these buffers are filled, subsequent
calls to the \method{Write} or \method{WriteAsync} method will block until some data are
acknowledged. The application data must be buffered until the other endpoint acknowledges their
reception. This takes at least one roundtrip, i.e., \SI{25}{\milli\second} in our test. Our
implementation can, therefore, send at most $8 \cdot 16 = 128 \si{\kibi\byte}$ on a single stream during
a round trip period. This sending rate implies a theoretical maximum throughput as
$128\si{\kibi\byte} \div 0.025 = 5 \si[per-mode=symbol]{\mebi\byte\per\second}$.

This hypothesis can be tested by doubling the delay on the simulated network.
\autoref{fig:04-lab-loss-throughput-long} shows measurements with \SI{50}{\milli\second} latencies
(\autoref{fig:04-lab-loss-throughput-a-50}) and \SI{100}{\milli\second} latencies
(\autoref{fig:04-lab-loss-throughput-a-100}). In these variations of the test we see that the
throughput of the managed implementation is highly correlated with the network latency as it halves
everytime the latency is doubled.

\begin{myFigure}{fig:04-lab-loss-throughput-long}{Single stream throughput measurements in simulated network with long delays (Linux LAN)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-loss-throughput-a-50}{\SI{50}{\milli\second} latency, 0.004\% loss}
\footnotesize
\input{plots/04-lab-loss-throughput-a-50.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-loss-throughput-a-100}{\SI{100}{\milli\second} latency, 0.004\% loss}
\footnotesize
\input{plots/04-lab-loss-throughput-a-100.tex}
\end{mySubfigure}
\end{myFigure}

This performance limitation could be fixed simply by increasing the number of buffers used to buffer
application data. However, a large number of such buffers would lead to inefficient memory
utilization in low-latency environments like a LAN network. Instead, the amount of data buffered
should be changed dynamically based on information such as the congestion window's size or the flow
control limit advertised by the other endpoint. We leave the implementation of such dynamic
buffering for future work.

\autoref{fig:04-lab-loss-latency} shows the latency measurements results. Each implementation
exhibits the same latency for both 0.004\% (\autoref{fig:04-lab-loss-latency-a}) and 0.1\%
(\autoref{fig:04-lab-loss-latency-b}) packet loss. This means that even 0.1\% packet loss rate is
still low enough to not affect the 99th percentile of latency.

\begin{myFigure}{fig:04-lab-loss-latency}{Single stream latency measurements in simulated network (Linux LAN)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-loss-latency-a}{\SI{25}{\milli\second} latency, 0.004\% loss}
\footnotesize
\input{plots/04-lab-loss-latency-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-loss-latency-b}{\SI{25}{\milli\second} latency, 0.1\% loss}
\footnotesize
\input{plots/04-lab-loss-latency-b.tex}
\end{mySubfigure}
\end{myFigure}

All implementations show almost the same latency in almost all tests. The single exception is our
implementation, which exhibits greater latencies for \SI{4096}{\byte} messages. This is because this
message size does not fit a single packet and our \gls{packet-pacing} implementation (see
\autoref{sec:05-recovery}) spaces out individual packets one by one and thus increases the delay
between sending the first and last QUIC packet carrying the \SI{4096}{\byte} message. The
\libmsquic{}-based implementation, on the other hand, sends small batches of packets for each tick
of the pacer and the small batches are enough to transmit the entire \SI{4096}{\byte} message.

By using only a single stream in each QUIC connection in these tests, it could be argued that we did
not utilize one of the big advantages of QUIC over TCP\@ --- the reduced \gls{head-of-line-blocking}.
\autoref{fig:04-lab-multi-loss-throughput} shows the measurements of throughput like we did
previously (for \autoref{fig:04-lab-loss-throughput}), but with QUIC connections transmitting data
using 32 parallel streams. Contrary to the expectation, the throughput of the QUIC implementations
increases only slightly.

Because we were sending data using multiple streams, the managed QUIC implementation was no longer
blocked by the limited buffering discussed above. Instead, investigation shows that it was limited
by the size of the congestion window. After more investigation, it turned out that the throughput of
both QUIC implementation was very high at the beginning, possibly even higher than that of TCP\@.
However, during the \SI{5}{\second} warmup period, the packet loss led to a significant reduction of
the congestion window, and the throughput was reduced to the values we can see on the plot.

\begin{myFigure}{fig:04-lab-multi-loss-throughput}{Multiple stream throughput measurements in simulated network (Linux LAN)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-loss-throughput-a}{32 streams, \SI{25}{\milli\second} latency, 0.004\% loss}
\footnotesize
\input{plots/04-lab-multi-loss-throughput-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-lab-multi-loss-throughput-b}{32 streams, \SI{25}{\milli\second} latency, 0.1\% loss}
\footnotesize
\input{plots/04-lab-multi-loss-throughput-b.tex}
\end{mySubfigure}
\end{myFigure}

We believe that this behavior was caused by the too uniform distribution of the packet loss we used
in our testing environment. In an environment with uniform packet loss, almost all lost packets
induce a separate congestion event and reducing the congestion window. When packets are lost in
bursts --- as is more common in real-world networks --- the loss of multiple consecutive packets is
handled as a single congestion event, leading to less frequent reductions of the congestion window.

In this case, the protocol's behavior depends on the implementation of the congestion control
algorithm. The managed QUIC implementation uses an adaptation of the NewReno algorithm described by
the QUIC specification~\autocite[Section~7]{draft-ietf-quic-recovery}. The \libmsquic{}-based QUIC
implementation and TCP used the CUBIC algorithm~\cite{rfc8312}. Because of the substantial
difference in performance, we consider it worthwhile to experiment with different congestion control
algorithms for our implementation. Studying the behavior and implementation of TCP congestion
control in the Linux kernel should also provide useful insight for improving our implementation's
behavior. However, due to the complexity, we leave such experiments and investigations for future
work.

\section{Interop between QUIC Implementations}

The benchmarking application can also test the managed QUIC implementation's interoperability with
the \libmsquic{}-based implementation. This can be done by starting one application in
\texttt{server} mode using one implementation provider and another instance of the application in
\texttt{client} mode using the other provider.

We have tested both combination with following results:

\begin{itemize}

  \litem{Managed client and \libmsquic{} server} With a small number of connections, the two
implementations interoperated without any noticeable problems. However, starting the benchmarking
application with a very large number of parallel connection sometimes made \libmsquic{} server
respond with a Retry packet for some connections. This is because \libmsquic{} engages client
address validation (see \autoref{sec:02-address-validation}) if there are too many simultaneous
connections attempts. Our implementation does not implement support for client address validation
via Retry packets, and, therefore, the connection was terminated immediately.

  \litem{\libmsquic{} client and Managed server} The version of the \libmsquic{} library we used in
our tests started all client connections using the draft 29 version of the protocol, while our
implementation supports only the draft 27 version. Because we did not implement version negotiation,
the server did not attempt negotiating the draft 27 protocol version, and the \libmsquic{} client
connections eventually closed due to idle timeout. Even if we decided to ignore the protocol version
in our implementation, these two versions use a different \textit{initial salt} for deriving Initial
packet protection keys (see \autoref{sec:02-encryption-key-derivation}). Therefore, our server would
drop all incoming packets because they could not be decrypted.

\end{itemize}

Because the only problems with interop between the two QUIC implementations were due to features we
explicitly decided to omit from our prototype implementation during our analysis in
\autoref{sec:03-feature-selection}, we are satisfied with these results.

Our benchmarking application did not perform an exhaustive interop test. A better interop tests
could be achieved using the open-source QUIC Interop Test Runner project~\cite{QuicInteropRunner}
which tests interoperabilty of popular QUIC implementations in various scenarios. However, this
level of testing is outside of the scope of this thesis and is subject to future work.

\section{Summary}

Compared to \libmsquic{}-based QUIC implementation provider, the managed QUIC implementation
developed as part of this thesis can provide higher throughput when using a small number of parallel
connections while maintaining comparable latencies. However, in the current state, the managed QUIC
implementation does not scale very well, and both throughput and observed latencies degrade when
large number of connections are created. When under substantial load, the \libmsquic{}-based
implementation delivered throughput up to four times greater than our implementation.

However, in our tests, the TCP+TLS stack has significantly larger throughput and lower latencies
than either QUIC implementation in our LAN evaluation environment. A possible explanation of this is
the presence of hardware optimizations for the TCP stack on the network interface card.

We also compared the QUIC and TCP implementations in a simulated 4G network. In our tests, we again
measured significantly higher throughput using TCP+TLS stack than either QUIC implementation, even
if we used multiple streams in QUIC connections. As for latencies, all implementations exhibit
almost identical latency, except for our implementation, which had larger latencies once the
messages exceeded the maximum size of a QUIC packet.

From our measurements, we conclude that our prototype QUIC implementation will need further
development and performance tuning in order to provide performance similar to that of \libmsquic{}.
We have identified some of the underlying problems in the current managed implementation and
proposed future improvements. However, both QUIC implementations will require a significant amount
of optimizations before reaching performance comparable to present TCP implementations.
