\chapter{Evaluation}\label{chap:04-evaluation}

In this chapter, we evaluate the performance of the managed QUIC implementation developed in this
thesis and compare it to the existing \libmsquic{}-based implementation. We will also compare both
managed and \libmsquic{}-based QUIC implementations to \class{SslStream} which uses a combination of
TCP and TLS\@. In our evaluation, we will focus on two performance characteristics relevant for
network communication:

\begin{itemize}

        \litem{Throughput} The rate at which the application data are sent. High total throughput is
        especially important for servers when handling large amount of parallel connections.

        \litem{Round Trip Time Latency} The delay between sending a request and receiving a
        response. This metric is important mainly for clients, where lower latencies mean faster
        responses from the server. Rather than measuring the average latency, it is more common to
        measure, e.g., 99th percentile of latency which indicates minimum expected latencies in the
        slowest 1\% replies. This metric is important in the context of webpages and communication
        with browsers. Many websites generate hundreds of HTTP requests for single page access which
        makes the probability of at least one such request being slower than the 99th percentile
        very high~\cite{Treat2015}. \todo{cite or deeply explain that this is a standard way of
        measuring it}

\end{itemize}

\section{Evaluation Environment}

Due to lack of resources for proper end-to-end testing on a real network, all performance tests were
run locally on a single Windows machine over the loopback network interface. The CPU on the machine
is Intel Core i7-8700 \SI{3.20}{\giga\hertz} (12 logical, 6 physical cores) and runs Windows
10.0.18363.1139. The program was run in a 64-bit process on preview \dotnet{}~6.0.0 host
(CoreCLR 6.0.20.55508, CoreFX 6.0.20.255508).

\subsection{Running with MsQuic Support}

\todo{this is hopefully less confusing now}

\libmsquic{}-based QUIC implementation does not work in the default installation of \dotnet{}
because \dotnet{} does not distribute the \libmsquic{} binary. Programs which want to use the
\libmsquic{}-based QUIC implementation should reference the \texttt{System.Net.Experimental.MsQuic}
NuGet package~\cite{SystemNetQuicExperimentalMsquic} which provides the necessary \libmsquic{}
binary. However, the Windows \libmsquic{} binary distributed in the NuGet package requires a Windows
Insider Preview build of Windows because it depends on newer \libschannel{} version for TLS
implementation which is not available in mainstream Windows version.

Unfortunately, we cannot install a preview build of Windows on the machine we will use for running
the tests. Fortunately, in order to support Linux, \libmsquic{} can also use the same modified
\libopenssl{} version for the TLS implementation as our managed QUIC implementation. Eventhough
building \libmsquic{} with \libopenssl{} for Windows is not officially supported, we were able to
produce such a build with small changes in the \libmsquic{} build configuration. Therefore, the
\libmsquic{} library used in experiments in this chapter has been compiled locally using source code
from commit \texttt{dc2a6cf0dd12e27} from the official \libmsquic{} repository~\cite{msquicGithub}
and configured to use the same \libopenssl{} library as the managed QUIC implementation.

It should be noted that our custom build of \libmsquic{} may show slightly different performance
characteristics than the official build for Windows which uses the \libschannel{} library. Also, all
measurements of the \libmsquic{}-based QUIC support for \dotnet{} include any overhead introduced by
the interop layer bridging the native library API to \dotnet{} QUIC API\@. The measured results,
therefore, do not represent raw \libmsquic{} library performance, but the performance observed by
\dotnet{} applications which use the \libmsquic{}-based implementation provider which can be validly
compared to the performance of the our managed QUIC implementation.

\subsection{Simulating Non-Ideal Network}

In order to measure how implementations behave in presence of lag and packet loss, we used the
Clumsy~\cite{clumsy} utility to simulate network problems. Clumsy is based on the
WinDivert~\cite{WinDivert} library for capturing packets from the Windows network stack.
\autoref{fig:04-clumsy-architecture} illustrates how Clumsy intercepts network traffic. The
\texttt{WinDivert.sys} kernel driver intercepts incoming packets and checks them against a set of
rules provided by Clumsy. Packets matching a rule are passed to Clumsy. Clumsy internally simulates
network issues and reinjects packets into the Windows network stack as necessary.

\begin{myFigure}{fig:04-clumsy-architecture}{Use of Clumsy to simulate network issues}

  \resizebox{0.8\linewidth}{!}{\input{img/04-clumsy-architecture.pdf_tex}}

\end{myFigure}

However, the implementation of Clumsy uses only a single thread to process packets. This means that
Clumsy becomes a bottleneck when processing large network load. \todo{mentioned that we could not find anything better than clumsy} Since we were unable to find a
better tool which would work on Windows, we will use Clumsy only in tests with a single connection
where the single-threaded implementation of Clumsy has the least effect on the measurements.

\subsection{Benchmarking Application}\label{sec:04-benchmark-app}

The actual throughput and latency measurements are done using a dedicated benchmarking \dotnet{}
application whose source code can be found in \todo{path to attachments}. The application implements
a trivial echo server and clients which exchange messages of specified size. The first parameter
to the application selects one of the following modes:

\begin{description}

  \ditem{\texttt{server}} Starts only the server-side part of the application.

  \ditem{\texttt{client}} Starts only the client-side part of the application. In this mode, the
application spawns multiple clients and reports the measurement results on the standard output.

  \ditem{\texttt{inproc}} Starts both the server and client in the same process. All network
communication is done over the loopback network interface.

\end{description}

The benchmarking application accepts multiple parameters. List of the most important follows

\begin{description}

    \ditem{\texttt{-e, --endpoint}} The endpoint on which to listen (server) or to which to connect (client). Not applicable for \texttt{inproc} mode.

    \ditem{\texttt{--certificate-file}} Path to the X.509 certificate file.

    \ditem{\texttt{--key-file}} Path to the X.509 certificate private key file.

    \ditem{\texttt{-t, --tcp}} Use TCP instead of QUIC\@.

    \ditem{\texttt{-c, --connections}} The number of connetions to create.

    \ditem{\texttt{-s, --streams}} The number of streams to create in each connection. Applicable only when QUIC is used.

    \ditem{\texttt{-m, --message-size}} The size of sent messages in bytes.

    \ditem{\texttt{-w, --warmup-time}} Time before starting to take measurements.

    \ditem{\texttt{-d, --test-duration}} Time after which the measurement should stop.

    \ditem{\texttt{-i, --reporting-interval}} Delay between reports intermediate measurements.

    \ditem{\texttt{-n --no-wait}} Whether clients wait for reply before sending another message.

\end{description}

The full list of parameters can be found in the \filename{Program.cs} file, or by running the
program with the \texttt{--help} parameter. By default, the implementation uses managed QUIC
implementation. Switching to \libmsquic{}-based implementation can be achieved by defining the
\texttt{DOTNETQUIC_PROVIDER} environment variable to \texttt{msquic}.

The client mode of the application switches between two behaviors. By default, clients do not wait
for reply from the server before sending another message. This lets us measure the total throughput
of the system. When using the \texttt{-n} switch, clients wait until a response from the server is
received and measure the latency as delay between sending the message and receiving the reply.

\section{Measurement Results}\label{sec:04-perf-results}

In the following subsections, we present the results of the individual performance experiments. The
throughput and latencies were measured as follows:

\begin{itemize}

  \litem{Throughput} Throughput of the entire server. This is the total amount of application data
  echoed back to clients across all connections.

  \litem{Latency} The delay between client writing the message to the \Stream{} and reading back the
full reply. In our tests, we will measure the 99th percentile of latency. The latency measurements
will be taken with the use of \texttt{-n} flag in the benchmark application.

\end{itemize}

We will perform three kinds of performance comparisions:

\begin{itemize}

  \litem{Multiple Stream Performance} These tests compare how the managed and \libmsquic{}-based
QUIC implementations scale with increasing server load.

  \litem{Single Stream Performance} These tests use only a single stream in a connection. These
tests should allow us to compare the performance between QUIC implementations and TCP+TLS-based
\SslStream{}.

  \litem{Single Stream Performance in Lossy Network} These are the only tests which will use Clumsy
to simulate a network with lag, packet loss and packet reordering. In these tests, we will observe
the behavior of a single connection with a single stream to avoid affecting the measurements by
single-threaded Clumsy implementation.

\end{itemize}

\todo{Explained why we chose the particular warmup and duration times and message sizes}
All experiments in this section were measured using the \texttt{inproc} mode of the application
described in previous section. Each test case had a \SI{5}{\second} warm-up time and collected data
for another \SI{15}{\second}. These intervals were long enough to produce stable measurements across
multiple test runs.

In the benchmarking application, clients and server exchange messages of size specified by the
\texttt{-m} parameter. In our tests, we will use mainly two message sizes: \SI{256}{\byte} and
\SI{4096}{\byte}. The \SI{256}{\byte} message are small enough to fit into a single QUIC packet,
while the bigger \SI{4096}{\byte} messages are guaranteed to be split across multiple QUIC packets.
Our expectation is that increasing the message size should increase the latency because more packets
need to be send to send both the message and the reply. Also, increasing the message should increase
throughput because there are fewer calls for queueing the same amount of data into the stream.

\subsection{Multiple Stream Performance Measurements}\label{sec:04-multi-stream-perf}

The first set of tests compared the managed and \libmsquic{}-based QUIC implementations. These tests
were run with increasingly larger messages and with greater number of parallel connections and
streams to see how the two implementations scale.

\todo{it is not certain what we are comparing.} \autoref{fig:04-multi-stream-throughput} shows the
measured throughput. \autoref{fig:04-multi-stream-throughput-a} shows the base-line performance when
\SI{256}{\byte} messages are sent using a single stream per connection. The other figures show
measurements after increasing message size to \SI{4096}{\byte}
(\autoref{fig:04-multi-stream-throughput-b}), increasing number of streams to 32
(\autoref{fig:04-multi-stream-throughput-c}), or both (\autoref{fig:04-multi-stream-throughput-d}).
An immediate observation can be made that the managed implementation produced very similar
measurements in all four test runs, with the highest throughput at 4 parallel connections and lowest
at 256. On the other hand, the \libmsquic{}-based implementation maintains the same throughput
regardless of the number of connections, but increases throughput both when increasing the message
size and number of streams in a connection.

\begin{myFigure}{fig:04-multi-stream-throughput}{Multiple stream QUIC throughput measurements}
\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-throughput-a}{1 Stream, \SI{256}{\byte} Messages}
\footnotesize
\input{plots/04-multi-stream-throughput-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-throughput-b}{1 Stream, \SI{4096}{\byte} Messages}
\footnotesize
\input{plots/04-multi-stream-throughput-b.tex}
\end{mySubfigure}

\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-throughput-c}{32 Stream, \SI{256}{\byte} Messages}
\footnotesize
\input{plots/04-multi-stream-throughput-c.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-throughput-d}{32 Stream, \SI{4096}{\byte} Messages}
\footnotesize
\input{plots/04-multi-stream-throughput-d.tex}
\end{mySubfigure}
\end{myFigure}

The peak of the throughput of our managed QUIC implementation at four connections can be explained
by the fact that we are spawning a long-running background task for each connection. Four
connections lead to a number of parallel \class{Task}s that were enough to completely utilize the
entire CPU\@.

As for the rapid decline of the throughput of managed QUIC implementation when 16 or more
connections were used, profiling showed that substantial amount of time was spent in the garbage
collection. At 256 connections, up to 50\% of the total CPU time was spent in GC, divided into
relatively large pauses of \SI{20}{\milli\second} or more. The pauses introduced by GC led to QUIC
packets being considered lost which in turn led to collapse of the congestion window in the QUIC
connection, further reducing the rate at which data were sent. This points to the fact that
eventhough we carefully avoided needless allocations in our implementation, there are still enough
sources of allocation left to degrade the performance for large numbers of connections.

An example of a large source of allocations is pooling the buffers for sending or receiving QUIC
packets. Even though the implementation uses \ArrayPoolOf{\Byte}\texttt{.Shared} to reuse allocated
buffers, this does not work well with large number of connections. This is because
\ArrayPoolOf{\Byte}\texttt{.Shared} retains only a small number of \ArrayOf{\Byte} instances for
pooling and lets GC collect the rest\footnote{In \dotnet{}~5, the implementation of
  \ArrayPoolOf{\Byte}\texttt{.Shared} maintains a separate pool for each CPU core. Each per-CPU core
  pool organizes pooled arrays into 17 buckets with array sizes ranging from \SI{16}{\byte} to
  \SI{1}{\mebi\byte}. Each bucket retains up to 8 array instances of similar size.}. With large
number of connections, the buffers are often rented and returned in large bursts, leading to a lot
of large arrays being discarded upon return to the \ArrayPoolOf{\Byte}\texttt{.Shared} and
subsequently allocated anew.

Another source of allocations are \Socket{}\texttt{.\method{SendTo}} and
\Socket{}\texttt{.\method{ReceiveFrom}} methods which allocate of a new \class{EndPoint} instance
for each call. In order to remove this particular source of allocations, an allocation-free API for
sending or receiving UDP datagrams needs to be designed and implemented on the \Socket{} class.

\libmsquic{}-based implementation, on the other hand, provides almost the same throughput regardless
the number of connections. Instead, it greatly increases with message size and only slightly with
the number of streams in the connections. When we tried sending messages larger than
\SI{4096}{\byte}, the total throughput did not increase significantly anymore. Upon closer
inspection of the \libmsquic{} network traffic when \SI{256}{\byte} messages are used, it turns out
that many of the sent packets are very small, possibly containing only \ACK{} frames. Thus, the
total available network bandwidth is not utilized fully. When \SI{4096}{\byte} messages are used,
more space in QUIC packets is utilized which increases the throughput. Our investigation, however,
did not uncover why the \libmsquic{} throughput does not scale with the number of connections.

\autoref{fig:04-multi-stream-latency} shows the measured latencies for the same four test cases as
above. In the baseline case (\autoref{fig:04-multi-stream-latency-a}), our QUIC implementation
outperforms the \libmsquic{}-based implementation, with less than half 99th latency percentile in
most cases. However, \libmsquic{}-based implementation outperforms our implementation in in tests
with increased message size (\autoref{fig:04-multi-stream-latency-b}), or greater number of streams
(\autoref{fig:04-multi-stream-latency-c}) or both (\autoref{fig:04-multi-stream-latency-d}),
especially when conducting large amounts of connections.

\begin{myFigure}{fig:04-multi-stream-latency}{Multiple stream QUIC latency measurements}
\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-latency-a}{1 Stream, \SI{256}{\byte} Messages}
\footnotesize
\input{plots/04-multi-stream-latency-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-latency-b}{1 Stream, \SI{4096}{\byte} Messages}
\footnotesize
\input{plots/04-multi-stream-latency-b.tex}
\end{mySubfigure}

\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-latency-c}{32 Stream, \SI{256}{\byte} Messages}
\footnotesize
\input{plots/04-multi-stream-latency-c.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-latency-d}{32 Stream, \SI{4096}{\byte} Messages}
\footnotesize
\input{plots/04-multi-stream-latency-d.tex}
\end{mySubfigure}
\end{myFigure}

Similarly to the degraded throughput discussed previously, the increased latency can be attributed
to the pauses introduced by GC\@. In the baseline test where our implementation outperforms
\libmsquic{}, the GC activity was under 5\%. It should be noted that when measuring latency, the
clients in the benchmarking application wait for the server to respond before sending another
message. This limits the amount of QUIC packets being sent which explains why our implementation
performs well in the baseline test even with 256 connections. Increasing either message size or the
number of streams significantly increases the network traffic which increases the pressure on GC in
our implementation.

Furthermore, by increasing the message size, we increased the total probability that the application
message will be delayed affected by the packet loss introduced by the GC pauses. This is because the
message is spread over multiple QUIC packets and loss of any of those packets will delay the entire
message.

Increasing the number of streams also increased the total network traffic, making our implementation
susceptible to the increased GC pauses. However, after certain threshold, increasing the number of
streams does not increase the throughput of the implementation anymore. Instead, it leads to
increase in latency, because it still increases the amount of outstanding parallel requests.

Lastly, we would like to remind that we were measuring the 99th percentile of the latency which
contains information only about the slowest 1\% of requests. To give more perspective on the latency
distribution on the two implementations, \autoref{fig:04-multi-stream-latency-median} shows the
median measurements of the latency. For brevity, we include only the measurements for
\SI{4096}{\byte} messages. The median measurements of latencies of our implementation are similar or
substantially lower than that of \libmsquic{}-based one.

\begin{myFigure}{fig:04-multi-stream-latency-median}{Multiple stream QUIC latency measurements (median)}
\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-latency-median-a}{1 Stream, \SI{4096}{\byte} Messages}
\footnotesize
\input{plots/04-multi-stream-latency-median-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-multi-stream-latency-median-b}{32 Stream, \SI{4096}{\byte} Messages}
\footnotesize
\input{plots/04-multi-stream-latency-median-b.tex}
\end{mySubfigure}
\end{myFigure}

We believe that this difference in the latency distribution between the two implementation is due to
the differences between their architecture. \libmsquic{} spawn several worker threads (the number
depends on the number of CPU cores) which process events for individual connections from an event
queue. The event queue ensures that the connections are serviced evenly and in stable intervals.
When the load on the server increases, the extra latency is spread evenly across all active
connections. This is further supported by the fact that the median values for the latency of
\libmsquic{} are only slightly lower than its 99th percentile from
\autoref{fig:04-multi-stream-latency}.

Our implementation, on the other hand, relies solely on the \dotnet{} \class{Task} scheduler which
schedules all background tasks independently and without any priority information. This allows for
greater variance in the latency of our QUIC implementation. In the future, the architecture of our
implementation should be improved to reduce the variance and, therefore, the 99th percentile of the
latency.

In summary, our QUIC implementation outperforms the \libmsquic{}-based one in scenarios where small
messages are exchanged between client and server using a small number of streams. However, our
implementation does not scale when when large messages are sent or when using larger number of
parallel streams. This is due to increased pressure on the GC which introduces frequent and large
pauses in the application, and the way background work is scheduled on the \dotnet{} thread-pool.

\subsection{Single Stream Performance}

In the second test, we compared the single-stream performance of QUIC implementations and also the
performance of \TcpClient{} and \SslStream{}. \autoref{fig:04-single-stream-throughput} shows the
results of the throughput measurements for \SI{256}{\byte}
(\autoref{fig:04-single-stream-throughput-a}) and \SI{4096}{\byte}
(\autoref{fig:04-single-stream-throughput-b}) messages. TCP seems to perform significantly better
than either QUIC implementation in both variations of the experiment.

\begin{myFigure}{fig:04-single-stream-throughput}{Single stream throughput measurements}
\begin{mySubfigure}{0.49\linewidth}{fig:04-single-stream-throughput-a}{\SI{256}{\byte} Messages}
\footnotesize
\input{plots/04-single-stream-throughput-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-single-stream-throughput-b}{\SI{4096}{\byte} Messages}
\footnotesize
\input{plots/04-single-stream-throughput-b.tex}
\end{mySubfigure}
\end{myFigure}

\autoref{fig:04-single-stream-latency} shows the latency measurement results. These results show
again that TCP outperforms both QUIC implementations. Also, the results show that the latency of
managed QUIC implementation degrades most rapidly from the three implementations.

\begin{myFigure}{fig:04-single-stream-latency}{Single stream latency measurements}
\begin{mySubfigure}{0.49\linewidth}{fig:04-single-stream-latency-a}{\SI{256}{\byte} Messages}
\footnotesize
\input{plots/04-single-stream-latency-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-single-stream-latency-b}{\SI{4096}{\byte} Messages}
\footnotesize
\input{plots/04-single-stream-latency-b.tex}
\end{mySubfigure}
\end{myFigure}

The results above suggest that neither QUIC implementation can compete with the \SslStream{}
performance over the loopback interface and simple swapping the use of \SslStream{} for
\QuicStream{}s is likely to degrade performance of the resulting application.

\subsection{Single Stream Performance in Lossy Network}

\todo{Explain the choice of parameters (everywhere!), why 5\% loss, what is a normal expected
  latency time...}

In the last test, we compared the resilience of QUIC and TCP+TLS implementations in a simulated
lossy network using Clumsy. Although Clumsy allows setting very small lag delays, in practice, the
lowest round trip we observed when using the \texttt{ping} utility averaged around
\SI{50}{\milli\second}. We will, therefore, assume average lag of \SI{25}{\milli\second} in our
measurements. Additionally to the lag, we added a constant 5\% chance of packet reordering and
varied the packet loss chance in our tests.

\todo{ping where?}

\todo{explain why we would want to simulate lower latency, what is a normal latency for reaching out to common websites?}

\todo{explain the percentages for packet reordering and loss}

\autoref{fig:04-loss-throughput} shows the measured throughput for 0\% packet loss
(\autoref{fig:04-loss-throughput-a}) and 1\% packet loss (\autoref{fig:04-loss-throughput-b}). The
measured result suggest that both QUIC providers are far behind the throughput of the TCP protocol.

\begin{myFigure}{fig:04-loss-throughput}{Single stream throughput measurements in simulated network}
\begin{mySubfigure}{0.49\linewidth}{fig:04-loss-throughput-a}{\SI{25}{\milli\second} Lag, 0\% Loss, 5\% Reorder}
\footnotesize
\input{plots/04-loss-throughput-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-loss-throughput-b}{\SI{25}{\milli\second} Lag, 1\% Loss, 5\% Reorder}
\footnotesize
\input{plots/04-loss-throughput-b.tex}
\end{mySubfigure}
\end{myFigure}

\autoref{fig:04-loss-throughput} shows the latency measurements results. In absence of packet loss
(\autoref{fig:04-loss-latency-a}), \libmsquic{}-based implementation produces lower latencies than
the other implementations. In lossy network (\autoref{fig:04-loss-latency-b}), both QUIC
implementations produce lower latencies than TCP\@.

\begin{myFigure}{fig:04-loss-latency}{Single stream latency measurements in simulated network}
\begin{mySubfigure}{0.49\linewidth}{fig:04-loss-latency-a}{\SI{25}{\milli\second} Lag, 0\% Loss, 5\% Reorder}
\footnotesize
\input{plots/04-loss-latency-a.tex}
\end{mySubfigure}
\begin{mySubfigure}{0.49\linewidth}{fig:04-loss-latency-b}{\SI{25}{\milli\second} Lag, 1\% Loss, 5\% Reorder}
\footnotesize
\input{plots/04-loss-latency-b.tex}
\end{mySubfigure}
\end{myFigure}

Investigation on the behavior of the managed QUIC implementation in these tests showed that even in
complete absence of explicit packet loss in Clumsy, some packets were still lost. This loss was an
effect of packet bursts produced by Clumsy because it reinjected packets back into the network stack
in large batches, which led to overflowing the receive buffers in OS socket implementation and
discarding of some of the packets. We believe that these bursts highly contributed to the observed
throughput degradation observed in both QUIC implementations.

\todo{I can test perf over WLAN between my desktop and laptop, but it is very unbalanced}

\todo{Possibly, I could remote into the university lab (Windows needed I can copy the binaries,
  linux could require compiling from source which would prove difficult) or ask at work}

\todo{I can test perf over WLAN between my desktop and laptop, but it is very unbalanced}

\todo{Possibly, I could remote into the university lab (Windows needed I can copy the binaries,
  linux could require compiling from source which would prove difficult) or ask at work}

\todo{Testing on Linux on MFF lab is possible, but I cannot modify loss (requires root access).
  Also, there are some problems when running on linux (app hanging in ThreadHelper.ThreadStart()),
  but I think I can work around it, but running 256 connections seems unreal}

\section{Interop between QUIC Implementations}

The benchmarking application can also be used to test interoperability of the managed QUIC
implementation and the \libmsquic{}-based implementation. This can be done by starting one
application in \texttt{server} mode using one implementation provider, and another instance of the
application in \texttt{client} mode using the other provider.

More exhaustive interop tests could be achieved using the open source QUIC Interop Test
Runner~\cite{QuicInteropRunner}. However, that sort of testing is left for future work.

\section{Conclusion}

\todo{use different section name?}

When compared to \libmsquic{}-based QUIC implementation provider, the managed QUIC implementation
developed in this thesis can provide higher throughput when using small number of parallel
connection while maintaining comparable latencies. However, in the current state, the managed QUIC
implementation does not scale very well and the throughput and observed latencies degrade when large
number of connections is created.

Both QUIC implementation providers have measured lower single-stream performance when compared with
the TCP+TLS stack over the loopback interface. However, it should be noted that the implementations
may behave differently when used over real network.

In case of simulated lossy environment, we measured significantly higher single-stream throughput
using TCP+TLS stack than either QUIC implementation. For smaller messages, the measured latencies
were similar in all implementations, but for larger messages, both QUIC implementations showed
significantly lower latencies than the combination of TCP+TLS.

We conclude from these measurements that while QUIC both implementations show some interesting
results, there is still a lot of optimizations required before their performance is comparable to
that of the TCP protocol.
